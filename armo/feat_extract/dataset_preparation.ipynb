{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initailization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = {\n",
    "    \"macro_area_ratio\": \"the ratio of total macros area in the layout\",\n",
    "    \"mean_macro_edge_length\": \"the edge length of all macros calculated by average\",\n",
    "    \"mean_macro_neighbor_distance\": \"the total near distances of macros calculated by average\",\n",
    "    \"min_rudy\": \"the minimum value of the rudy map\",\n",
    "    \"max_rudy\": \"the maximum value of the rudy map\",\n",
    "    \"mean_rudy\": \"the average of the rudy map\",\n",
    "    \"std_rudy\": \"the standard deviation of the rudy map\",\n",
    "    \"PAR_rudy\": \"the Peak-to-Average Ratio of the rudy map\",\n",
    "    \"high_density_rudy_ratio\": \"the ratio of hotspots area in the rudy map\",\n",
    "    \"min_rudy_pin\": \"the minimum value of the rudy pin map\",\n",
    "    \"max_rudy_pin\": \"the maximum value of the rudy pin map\",\n",
    "    \"mean_rudy_pin\": \"the average of the rudy pin map\",\n",
    "    \"std_rudy_pin\": \"the standard deviation of the rudy pin map\",\n",
    "    \"PAR_rudy_pin\": \"the Peak-to-Average Ratio of the rudy pin map\",\n",
    "    \"high_density_rudy_pin_ratio\": \"the ratio of hotspots area in the rudy pin map\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_func_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_design = [\"RISCY-a\", \"RISCY-b\", \"RISCY-FPU-a\", \"RISCY-FPU-b\"]\n",
    "test_design_a = [\"zero-riscy-a\"]\n",
    "test_design_b = [\"zero-riscy-b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"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\"\n",
    "\n",
    "def encode_image(features):\n",
    "    features_b64 = []\n",
    "    for image in features:\n",
    "        buff = BytesIO()\n",
    "        image.save(buff, format=\"PNG\")\n",
    "        buff.seek(0)\n",
    "        image_b64 = base64.b64encode(buff.read()).decode()\n",
    "        features_b64.append(image_b64)\n",
    "    return features_b64\n",
    "\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felixchaotw/mllm-physical-design/feat_extract/models/gpdl.py:163: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weight = torch.load(pretrained, map_location='cpu')['state_dict']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPDL(\n",
       "  (encoder): Encoder(\n",
       "    (c1): conv(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (c2): conv(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (c3): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv1): conv(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (upc1): upconv(\n",
       "      (main): Sequential(\n",
       "        (0): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (conv2): conv(\n",
       "      (main): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (upc2): upconv(\n",
       "      (main): Sequential(\n",
       "        (0): ConvTranspose2d(48, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): InstanceNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "        (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (conv3): Sequential(\n",
       "      (0): Conv2d(4, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models\n",
    "device = \"cuda:4\"\n",
    "opt = {'task': 'congestion_gpdl', 'save_path': 'work_dir/congestion_gpdl/', 'pretrained': '/home/felixchaotw/CircuitNet/model/congestion.pth', 'max_iters': 200000, 'plot_roc': False, 'arg_file': None, 'cpu': False, 'dataroot': '../../training_set/congestion', 'ann_file_train': './files/train_N28.csv', 'ann_file_test': './files/test_N28.csv', 'dataset_type': 'CongestionDataset', 'batch_size': 16, 'aug_pipeline': ['Flip'], 'model_type': 'GPDL', 'in_channels': 3, 'out_channels': 1, 'lr': 0.0002, 'weight_decay': 0, 'loss_type': 'MSELoss', 'eval_metric': ['NRMS', 'SSIM', 'EMD'], 'ann_file': './files/test_N28.csv', 'test_mode': True}\n",
    "model = models.__dict__[\"GPDL\"](**opt)\n",
    "model.init_weights(**opt)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 4\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests, base64\n",
    "import json\n",
    "import argparse\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import heapq\n",
    "import re\n",
    "\n",
    "\n",
    "tile_size = 16\n",
    "top_k = 20\n",
    "image_size = 256\n",
    "\n",
    "\n",
    "def get_label(label_path):\n",
    "    with open(label_path, 'r') as f:\n",
    "        logs = f.read()\n",
    "    matches = re.findall(r\"Total overcon =\\s*([\\d.]+)\", logs)\n",
    "    if matches:\n",
    "        ans = float(matches[-1])\n",
    "    else:\n",
    "        ans = 0\n",
    "        \n",
    "    return ans\n",
    "\n",
    "file_path = '/data2/NVIDIA/CircuitNet-N28/Dataset/congestion/feature/zero-riscy-a/7228-zero-riscy-a-1-c2-u0.9-m2-p4-f0.npy'\n",
    "label_path = '/data2/NVIDIA/CircuitNet-N28/Dataset/logs/7228-zero-riscy-a-1-c2-u0.9-m2-p4-f0'\n",
    "numpy_image = np.load(file_path)\n",
    "batch_image = numpy_image.transpose(2,0,1)\n",
    "image_features = []\n",
    "image_inferences = []\n",
    "\n",
    "for i, image in enumerate(batch_image):\n",
    "    image_features.append(image)\n",
    "    image_inferences.append(Image.fromarray(np.uint8(image * 255)))\n",
    "    \n",
    "get_label(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_features(image):\n",
    "    tiles_size = 2.25\n",
    "    image_height, image_width = image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    image = np.uint8(image*255)\n",
    "    \n",
    "    _, binary_image = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "\n",
    "    centroids = []\n",
    "    total_macros_area = 0\n",
    "    total_edge_length = 0\n",
    "    \n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        total_macros_area += w * h\n",
    "        total_edge_length += 2 * (w + h)\n",
    "        centroid_x = x + w / 2\n",
    "        centroid_y = y + h / 2\n",
    "        centroids.append((centroid_x, centroid_y))\n",
    "    \n",
    "    neighbor_distances = []\n",
    "    for i, (x1, y1) in enumerate(centroids):\n",
    "        min_distance = float(\"inf\")  \n",
    "        for j, (x2, y2) in enumerate(centroids):\n",
    "            if i != j: \n",
    "                distance = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "        neighbor_distances.append(min_distance)\n",
    "    \n",
    "\n",
    "    if neighbor_distances and num_macros > 1:\n",
    "        mean_neighbor_distance = sum(neighbor_distances) / len(neighbor_distances)\n",
    "    else:\n",
    "        mean_neighbor_distance = 0.0\n",
    " \n",
    "    \n",
    "    return {\n",
    "            \"mean_macro_neighbor_distance\": float(mean_neighbor_distance * tiles_size),\n",
    "            \"macro_area_ratio\": total_macros_area / total_image_area,\n",
    "            \"mean_macro_edge_length\": (total_edge_length / num_macros) * tiles_size,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rudy_features(image):\n",
    "    total_area = image.shape[0] * image.shape[1]\n",
    "    max_rudy = np.max(image)\n",
    "    min_rudy = np.min(image)\n",
    "    mean_rudy = np.mean(image)\n",
    "    std_rudy = np.std(image)\n",
    "    par_rudy = max_rudy / mean_rudy\n",
    "    high_density_rudy_ratio = (image > mean_rudy).sum() /  total_area\n",
    "    \n",
    "    return {\n",
    "        \"mean_rudy\": mean_rudy,\n",
    "        \"std_rudy\": std_rudy,\n",
    "        \"PAR_rudy\": par_rudy,\n",
    "        \"high_density_rudy_ratio\": high_density_rudy_ratio,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rudy_pin_features(image):\n",
    "    total_area = image.shape[0] * image.shape[1]\n",
    "    max_rudy = np.max(image)\n",
    "    min_rudy = np.min(image)\n",
    "    mean_rudy = np.mean(image)\n",
    "    std_rudy = np.std(image)\n",
    "    par_rudy = max_rudy / mean_rudy\n",
    "    high_density_rudy_ratio = (image > mean_rudy).sum() /  total_area\n",
    "    \n",
    "    return {\n",
    "        \"mean_rudy_pin\": mean_rudy,\n",
    "        \"std_rudy_pin\": std_rudy,\n",
    "        \"PAR_rudy_pin\": par_rudy,\n",
    "        \"high_density_rudy_pin_ratio\": high_density_rudy_ratio,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(images):\n",
    "    macro_feature = images[0]\n",
    "    rudy_feature = images[1]\n",
    "    rudy_pin_feature = images[2]\n",
    "    \n",
    "    mf = macro_features(macro_feature)\n",
    "    rf = rudy_features(rudy_feature)\n",
    "    rpf = rudy_pin_features(rudy_pin_feature)\n",
    "    \n",
    "    final_features = {**mf, **rf, **rpf}\n",
    "    \n",
    "    for feat_func in feat_func_list:\n",
    "        feat = feat_func(images)\n",
    "        final_features.update(feat)\n",
    "        \n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitness Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_macro_neighbor_distance': 0.0,\n",
       " 'macro_area_ratio': 0.55419921875,\n",
       " 'mean_macro_edge_length': 1741.5,\n",
       " 'mean_rudy': np.float64(0.15478117255206314),\n",
       " 'std_rudy': np.float64(0.14191670718407573),\n",
       " 'PAR_rudy': np.float64(6.460734102938998),\n",
       " 'high_density_rudy_ratio': np.float64(0.492401123046875),\n",
       " 'mean_rudy_pin': np.float64(0.13647938888563046),\n",
       " 'std_rudy_pin': np.float64(0.12887029229064226),\n",
       " 'PAR_rudy_pin': np.float64(7.327113699475887),\n",
       " 'high_density_rudy_pin_ratio': np.float64(0.2754974365234375)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_features(image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def dataset_setting(designs):\n",
    "    df_list = []\n",
    "    for design in designs:\n",
    "        feature_path = f\"/data2/NVIDIA/CircuitNet-N28/Dataset/congestion/feature/{design}/\" \n",
    "        label_path = f\"/data2/NVIDIA/CircuitNet-N28/Dataset/logs/\"\n",
    "\n",
    "        labels = []\n",
    "        ids = []\n",
    "\n",
    "        for filename in tqdm(os.listdir(feature_path)):\n",
    "            file_path = os.path.join(label_path, filename)\n",
    "            log_file_path = file_path.replace(\".npy\", \"\")\n",
    "            try:\n",
    "                label = get_label(log_file_path)\n",
    "            except:\n",
    "                label = np.nan\n",
    "            ids.append(filename)\n",
    "            labels.append(label)\n",
    "            \n",
    "        df = pd.DataFrame({\"id\": ids,})\n",
    "\n",
    "        for filename in tqdm(os.listdir(feature_path)):\n",
    "            file_path = os.path.join(feature_path, filename)\n",
    "            numpy_image = np.load(file_path)\n",
    "            batch_image = numpy_image.transpose(2,0,1)\n",
    "            image_features = []\n",
    "            for i, image in enumerate(batch_image):\n",
    "                image_features.append(image)\n",
    "            \n",
    "            index = (df[\"id\"] == filename)\n",
    "            \n",
    "            all_features = get_all_features(image_features)\n",
    "            for key, value in all_features.items():\n",
    "                df.loc[index, key] = value\n",
    "                \n",
    "        \n",
    "        df['label'] = labels\n",
    "        df_list.append(df)\n",
    "        \n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2003/2003 [00:00<00:00, 10286.51it/s]\n",
      "100%|██████████| 2003/2003 [01:20<00:00, 24.92it/s]\n",
      "100%|██████████| 1858/1858 [00:00<00:00, 8821.45it/s]\n",
      "100%|██████████| 1858/1858 [01:17<00:00, 24.00it/s]\n",
      "100%|██████████| 1969/1969 [00:00<00:00, 7612.45it/s]\n",
      "100%|██████████| 1969/1969 [01:16<00:00, 25.73it/s]\n",
      "100%|██████████| 1248/1248 [00:00<00:00, 5323.94it/s]\n",
      "100%|██████████| 1248/1248 [00:53<00:00, 23.32it/s]\n",
      "100%|██████████| 2042/2042 [00:00<00:00, 14349.25it/s]\n",
      "100%|██████████| 2042/2042 [01:18<00:00, 26.06it/s]\n",
      "100%|██████████| 1122/1122 [00:00<00:00, 12252.71it/s]\n",
      "100%|██████████| 1122/1122 [00:45<00:00, 24.54it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = dataset_setting(train_design)\n",
    "test_df_a = dataset_setting(test_design_a)\n",
    "test_df_b = dataset_setting(test_design_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_df = train_df[[\"id\"] + list(feat_pool.keys()) + [\"label\"]]\n",
    "test_df_a = test_df_a[[\"id\"] + list(feat_pool.keys()) + [\"label\"]]\n",
    "test_df_b = test_df_b[[\"id\"] + list(feat_pool.keys()) + [\"label\"]]\n",
    "train_df[list(feat_pool.keys())] = scaler.fit_transform(train_df[list(feat_pool.keys())])\n",
    "test_df_a[list(feat_pool.keys())] = scaler.fit_transform(test_df_a[list(feat_pool.keys())]) \n",
    "test_df_b[list(feat_pool.keys())] = scaler.fit_transform(test_df_b[list(feat_pool.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df_a.reset_index(drop=True, inplace=True)\n",
    "test_df_b.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/train_df.csv\", index=False)\n",
    "test_df_a.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_a.csv\", index=False)\n",
    "test_df_b.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/train_df.csv\")\n",
    "test_df_a = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_a.csv\")\n",
    "test_df_b = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['label'].notna()]\n",
    "test_df_a = test_df_a[test_df_a['label'].notna()]\n",
    "test_df_b = test_df_b[test_df_b['label'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df_a.reset_index(drop=True, inplace=True)\n",
    "test_df_b.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rudy_gradient_variability</th>\n",
       "      <th>clustered_macro_distance_std</th>\n",
       "      <th>rudy_pin_clustering_coefficient</th>\n",
       "      <th>macro_density_gradient</th>\n",
       "      <th>macro_aspect_ratio_variance</th>\n",
       "      <th>macro_compactness_index</th>\n",
       "      <th>rudy_pin_compaction_ratio</th>\n",
       "      <th>macro_variability_coefficient</th>\n",
       "      <th>macro_symmetry_coefficient</th>\n",
       "      <th>...</th>\n",
       "      <th>rudy_intensity_symmetry_index</th>\n",
       "      <th>rudy_deviation_effect_index</th>\n",
       "      <th>demarcated_macro_proximity_index</th>\n",
       "      <th>macro_surface_irregularity_index</th>\n",
       "      <th>macro_rudy_boundary_interaction_index</th>\n",
       "      <th>pin_density_peak_contrast</th>\n",
       "      <th>rudy_pin_density_flux_index</th>\n",
       "      <th>high_density_rudy_ratio</th>\n",
       "      <th>high_density_rudy_pin_ratio</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>766-RISCY-a-2-c2-u0.7-m4-p8-f0.npy</td>\n",
       "      <td>0.489413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530802</td>\n",
       "      <td>0.120866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053970</td>\n",
       "      <td>0.177649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.405491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537237</td>\n",
       "      <td>0.400932</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.113466</td>\n",
       "      <td>0.099434</td>\n",
       "      <td>0.536391</td>\n",
       "      <td>0.499162</td>\n",
       "      <td>0.248046</td>\n",
       "      <td>0.656839</td>\n",
       "      <td>4.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>872-RISCY-a-2-c2-u0.75-m2-p1-f1.npy</td>\n",
       "      <td>0.493367</td>\n",
       "      <td>0.361191</td>\n",
       "      <td>0.574629</td>\n",
       "      <td>0.224413</td>\n",
       "      <td>0.132389</td>\n",
       "      <td>0.292900</td>\n",
       "      <td>0.299964</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.457941</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629108</td>\n",
       "      <td>0.297905</td>\n",
       "      <td>0.012593</td>\n",
       "      <td>0.503112</td>\n",
       "      <td>0.259475</td>\n",
       "      <td>0.620925</td>\n",
       "      <td>0.564001</td>\n",
       "      <td>0.278744</td>\n",
       "      <td>0.716711</td>\n",
       "      <td>6.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1710-RISCY-a-3-c5-u0.7-m1-p2-f1.npy</td>\n",
       "      <td>0.345073</td>\n",
       "      <td>0.568532</td>\n",
       "      <td>0.439762</td>\n",
       "      <td>0.387461</td>\n",
       "      <td>0.128708</td>\n",
       "      <td>0.398961</td>\n",
       "      <td>0.161277</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.112225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709836</td>\n",
       "      <td>0.357007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606688</td>\n",
       "      <td>0.238713</td>\n",
       "      <td>0.365033</td>\n",
       "      <td>0.380997</td>\n",
       "      <td>0.190162</td>\n",
       "      <td>0.608084</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216-RISCY-a-1-c2-u0.85-m3-p1-f1.npy</td>\n",
       "      <td>0.710298</td>\n",
       "      <td>0.306830</td>\n",
       "      <td>0.718464</td>\n",
       "      <td>0.225006</td>\n",
       "      <td>0.116272</td>\n",
       "      <td>0.188630</td>\n",
       "      <td>0.345568</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.449519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325532</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.381088</td>\n",
       "      <td>0.223520</td>\n",
       "      <td>0.419626</td>\n",
       "      <td>0.572189</td>\n",
       "      <td>0.332894</td>\n",
       "      <td>0.735863</td>\n",
       "      <td>13.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>313-RISCY-a-1-c5-u0.8-m1-p2-f0.npy</td>\n",
       "      <td>0.679618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631475</td>\n",
       "      <td>0.030455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.243119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685146</td>\n",
       "      <td>0.547051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043464</td>\n",
       "      <td>0.095405</td>\n",
       "      <td>0.508438</td>\n",
       "      <td>0.648357</td>\n",
       "      <td>0.335485</td>\n",
       "      <td>0.781597</td>\n",
       "      <td>6.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5592</th>\n",
       "      <td>6063-RISCY-FPU-b-1-c5-u0.9-m2-p5-f0.npy</td>\n",
       "      <td>0.528926</td>\n",
       "      <td>0.546966</td>\n",
       "      <td>0.839727</td>\n",
       "      <td>0.470527</td>\n",
       "      <td>0.077655</td>\n",
       "      <td>0.541395</td>\n",
       "      <td>0.810905</td>\n",
       "      <td>0.289318</td>\n",
       "      <td>0.373654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527353</td>\n",
       "      <td>0.596376</td>\n",
       "      <td>0.040966</td>\n",
       "      <td>0.609446</td>\n",
       "      <td>0.204903</td>\n",
       "      <td>0.643935</td>\n",
       "      <td>0.546314</td>\n",
       "      <td>0.355424</td>\n",
       "      <td>0.323224</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5593</th>\n",
       "      <td>6882-RISCY-FPU-b-3-c5-u0.85-m2-p4-f0.npy</td>\n",
       "      <td>0.655161</td>\n",
       "      <td>0.530913</td>\n",
       "      <td>0.517956</td>\n",
       "      <td>0.519424</td>\n",
       "      <td>0.470017</td>\n",
       "      <td>0.607458</td>\n",
       "      <td>0.060533</td>\n",
       "      <td>0.654525</td>\n",
       "      <td>0.080673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501380</td>\n",
       "      <td>0.645085</td>\n",
       "      <td>0.064977</td>\n",
       "      <td>0.607488</td>\n",
       "      <td>0.383442</td>\n",
       "      <td>0.290005</td>\n",
       "      <td>0.256680</td>\n",
       "      <td>0.419104</td>\n",
       "      <td>0.280389</td>\n",
       "      <td>7.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5594</th>\n",
       "      <td>5971-RISCY-FPU-b-1-c5-u0.7-m2-p1-f0.npy</td>\n",
       "      <td>0.214704</td>\n",
       "      <td>0.506835</td>\n",
       "      <td>0.567354</td>\n",
       "      <td>0.390982</td>\n",
       "      <td>0.279124</td>\n",
       "      <td>0.614118</td>\n",
       "      <td>0.269056</td>\n",
       "      <td>0.548690</td>\n",
       "      <td>0.193615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.758572</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.009570</td>\n",
       "      <td>0.795739</td>\n",
       "      <td>0.197851</td>\n",
       "      <td>0.355259</td>\n",
       "      <td>0.381986</td>\n",
       "      <td>0.566755</td>\n",
       "      <td>0.459280</td>\n",
       "      <td>4.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5595</th>\n",
       "      <td>6435-RISCY-FPU-b-2-c5-u0.75-m2-p6-f0.npy</td>\n",
       "      <td>0.485023</td>\n",
       "      <td>0.565166</td>\n",
       "      <td>0.541479</td>\n",
       "      <td>0.491429</td>\n",
       "      <td>0.185126</td>\n",
       "      <td>0.689024</td>\n",
       "      <td>0.260805</td>\n",
       "      <td>0.366085</td>\n",
       "      <td>0.197589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554700</td>\n",
       "      <td>0.481587</td>\n",
       "      <td>0.073707</td>\n",
       "      <td>0.780294</td>\n",
       "      <td>0.299667</td>\n",
       "      <td>0.615770</td>\n",
       "      <td>0.363502</td>\n",
       "      <td>0.388538</td>\n",
       "      <td>0.437832</td>\n",
       "      <td>3.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5596</th>\n",
       "      <td>6854-RISCY-FPU-b-3-c5-u0.8-m1-p8-f0.npy</td>\n",
       "      <td>0.749504</td>\n",
       "      <td>0.944246</td>\n",
       "      <td>0.455066</td>\n",
       "      <td>0.840499</td>\n",
       "      <td>0.129591</td>\n",
       "      <td>0.978620</td>\n",
       "      <td>0.226032</td>\n",
       "      <td>0.466883</td>\n",
       "      <td>0.297618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560525</td>\n",
       "      <td>0.487378</td>\n",
       "      <td>0.074043</td>\n",
       "      <td>0.898482</td>\n",
       "      <td>0.468136</td>\n",
       "      <td>0.390787</td>\n",
       "      <td>0.307107</td>\n",
       "      <td>0.322134</td>\n",
       "      <td>0.323224</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5597 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            id  rudy_gradient_variability  \\\n",
       "0           766-RISCY-a-2-c2-u0.7-m4-p8-f0.npy                   0.489413   \n",
       "1          872-RISCY-a-2-c2-u0.75-m2-p1-f1.npy                   0.493367   \n",
       "2          1710-RISCY-a-3-c5-u0.7-m1-p2-f1.npy                   0.345073   \n",
       "3          216-RISCY-a-1-c2-u0.85-m3-p1-f1.npy                   0.710298   \n",
       "4           313-RISCY-a-1-c5-u0.8-m1-p2-f0.npy                   0.679618   \n",
       "...                                        ...                        ...   \n",
       "5592   6063-RISCY-FPU-b-1-c5-u0.9-m2-p5-f0.npy                   0.528926   \n",
       "5593  6882-RISCY-FPU-b-3-c5-u0.85-m2-p4-f0.npy                   0.655161   \n",
       "5594   5971-RISCY-FPU-b-1-c5-u0.7-m2-p1-f0.npy                   0.214704   \n",
       "5595  6435-RISCY-FPU-b-2-c5-u0.75-m2-p6-f0.npy                   0.485023   \n",
       "5596   6854-RISCY-FPU-b-3-c5-u0.8-m1-p8-f0.npy                   0.749504   \n",
       "\n",
       "      clustered_macro_distance_std  rudy_pin_clustering_coefficient  \\\n",
       "0                         0.000000                         0.530802   \n",
       "1                         0.361191                         0.574629   \n",
       "2                         0.568532                         0.439762   \n",
       "3                         0.306830                         0.718464   \n",
       "4                         0.000000                         0.631475   \n",
       "...                            ...                              ...   \n",
       "5592                      0.546966                         0.839727   \n",
       "5593                      0.530913                         0.517956   \n",
       "5594                      0.506835                         0.567354   \n",
       "5595                      0.565166                         0.541479   \n",
       "5596                      0.944246                         0.455066   \n",
       "\n",
       "      macro_density_gradient  macro_aspect_ratio_variance  \\\n",
       "0                   0.120866                     0.000000   \n",
       "1                   0.224413                     0.132389   \n",
       "2                   0.387461                     0.128708   \n",
       "3                   0.225006                     0.116272   \n",
       "4                   0.030455                     0.000000   \n",
       "...                      ...                          ...   \n",
       "5592                0.470527                     0.077655   \n",
       "5593                0.519424                     0.470017   \n",
       "5594                0.390982                     0.279124   \n",
       "5595                0.491429                     0.185126   \n",
       "5596                0.840499                     0.129591   \n",
       "\n",
       "      macro_compactness_index  rudy_pin_compaction_ratio  \\\n",
       "0                    0.053970                   0.177649   \n",
       "1                    0.292900                   0.299964   \n",
       "2                    0.398961                   0.161277   \n",
       "3                    0.188630                   0.345568   \n",
       "4                    0.002034                   0.243119   \n",
       "...                       ...                        ...   \n",
       "5592                 0.541395                   0.810905   \n",
       "5593                 0.607458                   0.060533   \n",
       "5594                 0.614118                   0.269056   \n",
       "5595                 0.689024                   0.260805   \n",
       "5596                 0.978620                   0.226032   \n",
       "\n",
       "      macro_variability_coefficient  macro_symmetry_coefficient  ...  \\\n",
       "0                          0.000000                    0.405491  ...   \n",
       "1                          0.003904                    0.457941  ...   \n",
       "2                          0.006868                    0.112225  ...   \n",
       "3                          0.011228                    0.449519  ...   \n",
       "4                          0.000000                    0.112645  ...   \n",
       "...                             ...                         ...  ...   \n",
       "5592                       0.289318                    0.373654  ...   \n",
       "5593                       0.654525                    0.080673  ...   \n",
       "5594                       0.548690                    0.193615  ...   \n",
       "5595                       0.366085                    0.197589  ...   \n",
       "5596                       0.466883                    0.297618  ...   \n",
       "\n",
       "      rudy_intensity_symmetry_index  rudy_deviation_effect_index  \\\n",
       "0                          0.537237                     0.400932   \n",
       "1                          0.629108                     0.297905   \n",
       "2                          0.709836                     0.357007   \n",
       "3                          0.325532                     0.436585   \n",
       "4                          0.685146                     0.547051   \n",
       "...                             ...                          ...   \n",
       "5592                       0.527353                     0.596376   \n",
       "5593                       0.501380                     0.645085   \n",
       "5594                       0.758572                     0.187900   \n",
       "5595                       0.554700                     0.481587   \n",
       "5596                       0.560525                     0.487378   \n",
       "\n",
       "      demarcated_macro_proximity_index  macro_surface_irregularity_index  \\\n",
       "0                             0.000839                          0.113466   \n",
       "1                             0.012593                          0.503112   \n",
       "2                             0.000000                          0.606688   \n",
       "3                             0.003022                          0.381088   \n",
       "4                             0.000000                          0.043464   \n",
       "...                                ...                               ...   \n",
       "5592                          0.040966                          0.609446   \n",
       "5593                          0.064977                          0.607488   \n",
       "5594                          0.009570                          0.795739   \n",
       "5595                          0.073707                          0.780294   \n",
       "5596                          0.074043                          0.898482   \n",
       "\n",
       "      macro_rudy_boundary_interaction_index  pin_density_peak_contrast  \\\n",
       "0                                  0.099434                   0.536391   \n",
       "1                                  0.259475                   0.620925   \n",
       "2                                  0.238713                   0.365033   \n",
       "3                                  0.223520                   0.419626   \n",
       "4                                  0.095405                   0.508438   \n",
       "...                                     ...                        ...   \n",
       "5592                               0.204903                   0.643935   \n",
       "5593                               0.383442                   0.290005   \n",
       "5594                               0.197851                   0.355259   \n",
       "5595                               0.299667                   0.615770   \n",
       "5596                               0.468136                   0.390787   \n",
       "\n",
       "      rudy_pin_density_flux_index  high_density_rudy_ratio  \\\n",
       "0                        0.499162                 0.248046   \n",
       "1                        0.564001                 0.278744   \n",
       "2                        0.380997                 0.190162   \n",
       "3                        0.572189                 0.332894   \n",
       "4                        0.648357                 0.335485   \n",
       "...                           ...                      ...   \n",
       "5592                     0.546314                 0.355424   \n",
       "5593                     0.256680                 0.419104   \n",
       "5594                     0.381986                 0.566755   \n",
       "5595                     0.363502                 0.388538   \n",
       "5596                     0.307107                 0.322134   \n",
       "\n",
       "      high_density_rudy_pin_ratio  label  \n",
       "0                        0.656839   4.70  \n",
       "1                        0.716711   6.29  \n",
       "2                        0.608084   4.75  \n",
       "3                        0.735863  13.45  \n",
       "4                        0.781597   6.68  \n",
       "...                           ...    ...  \n",
       "5592                     0.323224  11.69  \n",
       "5593                     0.280389   7.05  \n",
       "5594                     0.459280   4.61  \n",
       "5595                     0.437832   3.90  \n",
       "5596                     0.323224   3.86  \n",
       "\n",
       "[5597 rows x 27 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_design(name):\n",
    "    for d in train_design:\n",
    "        if d in name:\n",
    "            return d\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"design\"] = train_df[\"id\"].apply(id_to_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1169/1169 [1:44:03<00:00,  5.34s/it] \n",
      "100%|██████████| 1248/1248 [1:29:33<00:00,  4.31s/it]\n",
      "100%|██████████| 1322/1322 [2:00:08<00:00,  5.45s/it] \n",
      "100%|██████████| 1858/1858 [8:50:32<00:00, 17.13s/it]  \n"
     ]
    }
   ],
   "source": [
    "preference_df_list = []\n",
    "num_pairs = 50000\n",
    "\n",
    "for design, group in train_df.groupby(\"design\"):\n",
    "    preference_df = pd.DataFrame(columns=[\"design\", \"chosen\", \"rejected\", \"chosen_score\", \"rejected_score\"])\n",
    "    group = group.reset_index(drop=True)\n",
    "    num_samples = len(group)\n",
    "    for i in tqdm(range(0, num_samples)):\n",
    "        for j in range(i+1, num_samples):\n",
    "            sample_a = group.iloc[i]\n",
    "            sample_b = group.iloc[j]\n",
    "            if sample_a[\"label\"] > sample_b[\"label\"]:\n",
    "                chosen = sample_a[\"id\"]\n",
    "                rejected = sample_b[\"id\"]\n",
    "                chosen_score = sample_a[\"label\"]\n",
    "                rejected_score = sample_b[\"label\"]\n",
    "            else:\n",
    "                chosen = sample_b[\"id\"]\n",
    "                rejected = sample_a[\"id\"]\n",
    "                chosen_score = sample_b[\"label\"]\n",
    "                rejected_score = sample_a[\"label\"]\n",
    "                \n",
    "            preference_df = preference_df._append({\"design\": design, \"chosen\": chosen, \"rejected\": rejected, \"chosen_score\": chosen_score, \"rejected_score\": rejected_score}, ignore_index=True)\n",
    "            \n",
    "    preference_df = preference_df.sample(frac=1).reset_index(drop=True)\n",
    "    preference_df = preference_df.sample(n=num_pairs)\n",
    "    preference_df_list.append(preference_df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_df = pd.concat(preference_df_list)\n",
    "preference_df.reset_index(drop=True, inplace=True)\n",
    "preference_df.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/preference_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 1 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_intersection_density(images):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to uint8 and binary\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Define RUDY high-density regions as those with values above a threshold\n",
    "    rudy_thresh = 0.5  # example threshold for high density \n",
    "    rudy_high_density = (rudy_image > rudy_thresh).astype(np.uint8)\n",
    "    \n",
    "    # Find macro intersections with RUDY high-density regions\n",
    "    macro_intersections = 0\n",
    "    for contour in contours:\n",
    "        mask = np.zeros_like(macro_image, dtype=np.uint8)\n",
    "        cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "        \n",
    "        intersection = cv2.bitwise_and(mask, mask, mask=rudy_high_density * 255)\n",
    "        if cv2.countNonZero(intersection) > 0:\n",
    "            macro_intersections += 1\n",
    "\n",
    "    # Calculate the density of macro intersections\n",
    "    feature_value = (macro_intersections * tiles_size) / (total_image_area * tiles_size)\n",
    "    \n",
    "    return {\"macro_intersection_density\": feature_value}\n",
    "\n",
    "\n",
    "def rudy_gradient_variability(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0, 255] range\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "\n",
    "    # Compute gradients using the Sobel operator\n",
    "    grad_x = cv2.Sobel(rudy_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(rudy_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    magnitude = cv2.magnitude(grad_x, grad_y)\n",
    "\n",
    "    # Calculate gradient variability\n",
    "    gradient_variability = np.var(magnitude)\n",
    "\n",
    "    feature_value = gradient_variability * (tiles_size ** 2)  # Convert to um^2\n",
    "\n",
    "    return {\"rudy_gradient_variability\": feature_value}\n",
    "\n",
    "def macro_perimeter_ratio(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    \n",
    "    # Convert the macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold to create a binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate the total perimeter length of macros\n",
    "    macro_perimeter = 0\n",
    "    for contour in contours:\n",
    "        macro_perimeter += cv2.arcLength(contour, True)\n",
    "    \n",
    "    # Convert pixel perimeter length to micrometers\n",
    "    macro_perimeter_um = macro_perimeter * tiles_size\n",
    "    \n",
    "    # Calculate the perimeter length of the entire image\n",
    "    layout_perimeter_um = (2 * (256 * tiles_size) + 2 * (256 * tiles_size))\n",
    "    \n",
    "    # Calculate the macro perimeter ratio\n",
    "    macro_perimeter_ratio = macro_perimeter_um / layout_perimeter_um\n",
    "    \n",
    "    return {\"macro_perimeter_ratio\": macro_perimeter_ratio}\n",
    "\n",
    "\n",
    "def clustered_macro_distance_std(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to 0-255\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    # Threshold and find contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Calculate centers of mass for each contour (macro)\n",
    "    centers = []\n",
    "    for contour in contours:\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            centers.append((cX, cY))\n",
    "\n",
    "    # Calculate pairwise distances\n",
    "    distances = []\n",
    "    num_centers = len(centers)\n",
    "    for i in range(num_centers):\n",
    "        for j in range(i + 1, num_centers):\n",
    "            dist = np.sqrt((centers[i][0] - centers[j][0]) ** 2 + (centers[i][1] - centers[j][1]) ** 2)\n",
    "            distances.append(dist * tiles_size)  # Convert pixel distance to um\n",
    "\n",
    "    # Calculate standard deviation of distances\n",
    "    if distances:\n",
    "        std_distance = np.std(distances)\n",
    "    else:\n",
    "        std_distance = 0.0\n",
    "    \n",
    "    return {\"clustered_macro_distance_std\": std_distance}\n",
    "\n",
    "\n",
    "def macro_proximity_variance(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to [0-255] range\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Detect high-density Rudy pin areas (assuming high density is > 0.5)\n",
    "    high_density_mask = rudy_pin_image > 0.5\n",
    "    \n",
    "    # Calculate proximity variance\n",
    "    distances = []\n",
    "    for contour in contours:\n",
    "        # Compute the centroid of each macro\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            \n",
    "            # Find the distance to the nearest high-density Rudy area\n",
    "            distances_to_high_density = []\n",
    "            for i in range(high_density_mask.shape[0]):\n",
    "                for j in range(high_density_mask.shape[1]):\n",
    "                    if high_density_mask[i, j]:\n",
    "                        dist = np.sqrt((cX - j) ** 2 + (cY - i) ** 2)\n",
    "                        distances_to_high_density.append(dist)\n",
    "            \n",
    "            if distances_to_high_density:\n",
    "                nearest_distance = min(distances_to_high_density)\n",
    "                # Convert distance to micrometers\n",
    "                distances.append(nearest_distance * tiles_size)\n",
    "    \n",
    "    variance = np.var(distances) if distances else 0\n",
    "    \n",
    "    return {\"macro_proximity_variance\": variance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [macro_intersection_density, rudy_gradient_variability, macro_perimeter_ratio, clustered_macro_distance_std, macro_proximity_variance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_func_list = new_feat_func_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_feat_pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m desc\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mnew_feat_pool\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_feat_pool' is not defined"
     ]
    }
   ],
   "source": [
    "desc.update(new_feat_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 2 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def rudy_pin_clustering_coefficient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Convert rudy_pin_image to binary\n",
    "    _, rudy_pin_binary = cv2.threshold(rudy_pin_image, 0.5, 1, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of the rudy pins\n",
    "    contours, _ = cv2.findContours(rudy_pin_binary.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    total_rudy_pins = len(contours)\n",
    "    clustered_rudy_pins = 0\n",
    "    \n",
    "    # Calculate the clustering coefficient\n",
    "    for contour in contours:\n",
    "        if len(contour) > 1:\n",
    "            clustered_rudy_pins += 1\n",
    "            \n",
    "    clustering_coefficient = clustered_rudy_pins / total_rudy_pins if total_rudy_pins > 0 else 0\n",
    "    \n",
    "    return {\"rudy_pin_clustering_coefficient\": clustering_coefficient}\n",
    "\n",
    "\n",
    "def macro_connectivity_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to binary\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Initialize feature value\n",
    "    connectivity_count = 0\n",
    "    \n",
    "    # Analyze bounding rectangles for each contour to find direct neighbors\n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "    \n",
    "    # Check for connections between bounding boxes\n",
    "    for i, (x1, y1, w1, h1) in enumerate(bounding_boxes):\n",
    "        for j, (x2, y2, w2, h2) in enumerate(bounding_boxes):\n",
    "            if i != j:\n",
    "                # Check for adjacency (consider the margins as connected)\n",
    "                if (abs(x1 - x2) <= tiles_size and (y1 <= y2 <= y1 + h1 or y2 <= y1 <= y2 + h2)) or \\\n",
    "                   (abs(y1 - y2) <= tiles_size and (x1 <= x2 <= x1 + w1 or x2 <= x1 <= x2 + w2)):\n",
    "                    connectivity_count += 1\n",
    "    \n",
    "    # Each connection is counted twice (one for each direction), divide by 2\n",
    "    macro_connectivity_index = connectivity_count / 2\n",
    "\n",
    "    return {\"macro_connectivity_index\": macro_connectivity_index}\n",
    "\n",
    "\n",
    "def edge_congestion_ratio(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Scale macro image to [0,255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Analyze RUDY image for congestion near edges\n",
    "    threshold_value = 0.5  # Define what \"high density\" means, can be adjusted\n",
    "    edge_band_width = 10   # Example width of edge band in pixels, can be adjusted\n",
    "    \n",
    "    # Define edge bands\n",
    "    left_edge = rudy_image[:, 0:edge_band_width]\n",
    "    right_edge = rudy_image[:, -edge_band_width:]\n",
    "    top_edge = rudy_image[0:edge_band_width, :]\n",
    "    bottom_edge = rudy_image[-edge_band_width:, :]\n",
    "    \n",
    "    # Calculate congestion in each edge\n",
    "    left_congestion = np.sum(left_edge > threshold_value)\n",
    "    right_congestion = np.sum(right_edge > threshold_value)\n",
    "    top_congestion = np.sum(top_edge > threshold_value)\n",
    "    bottom_congestion = np.sum(bottom_edge > threshold_value)\n",
    "\n",
    "    # Total edge congestion\n",
    "    total_edge_congestion = left_congestion + right_congestion + top_congestion + bottom_congestion\n",
    "    \n",
    "    # Convert pixel area to um^2\n",
    "    total_edge_congestion_area_um = total_edge_congestion * (tiles_size ** 2)\n",
    "    total_layout_area_um = total_image_area * (tiles_size ** 2)\n",
    "\n",
    "    # Calculate edge congestion ratio\n",
    "    edge_congestion_ratio_value = total_edge_congestion_area_um / total_layout_area_um\n",
    "    \n",
    "    return {\"edge_congestion_ratio\": edge_congestion_ratio_value}\n",
    "\n",
    "def macro_density_gradient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to binary [0, 255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours to get macro regions\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate macro density per region\n",
    "    macro_density = np.zeros((image_height, image_width))\n",
    "    for contour in contours:\n",
    "        mask = np.zeros_like(binary_image)\n",
    "        cv2.drawContours(mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "        macro_density += mask\n",
    "\n",
    "    # Gradient of the macro density\n",
    "    gradient_x = cv2.Sobel(macro_density, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    gradient_y = cv2.Sobel(macro_density, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    \n",
    "    # Calculate the magnitude of gradients\n",
    "    gradient_magnitude = cv2.magnitude(gradient_x, gradient_y)\n",
    "\n",
    "    # Calculate the average gradient magnitude in micrometers\n",
    "    macro_density_gradient_um = np.sum(gradient_magnitude) / (image_height * image_width)\n",
    "    macro_density_gradient_um *= tiles_size  # Convert to micrometers\n",
    "\n",
    "    return {\"macro_density_gradient\": macro_density_gradient_um}\n",
    "\n",
    "\n",
    "def macro_distribution_evenness(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to uint8 from [0-1] to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold the macro image to binary\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate convex hull of the macros\n",
    "    hull_area = 0\n",
    "    for contour in contours:\n",
    "        hull = cv2.convexHull(contour)\n",
    "        hull_area += cv2.contourArea(hull)\n",
    "    \n",
    "    # Normalize the hull area to (um^2)\n",
    "    hull_area_um2 = hull_area * (tiles_size ** 2)\n",
    "    \n",
    "    # Total layout area in um^2\n",
    "    total_layout_area_um2 = total_image_area * (tiles_size ** 2)\n",
    "    \n",
    "    # Evenness metric\n",
    "    if total_layout_area_um2 > 0:\n",
    "        evenness = hull_area_um2 / total_layout_area_um2\n",
    "    else:\n",
    "        evenness = 0\n",
    "    \n",
    "    return {\"macro_distribution_evenness\": evenness}\n",
    "\n",
    "\n",
    "def rudy_pin_sparsity_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height * (tiles_size ** 2)  # Convert pixels to um²\n",
    "\n",
    "    # Convert macro image to [0-255] and find contours\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate RUDY pin density\n",
    "    rudy_pin_density = np.sum(rudy_pin_image) / total_image_area\n",
    "\n",
    "    # Calculate RUDY density\n",
    "    rudy_density = np.sum(rudy_image) / total_image_area\n",
    "\n",
    "    # Rudy pin sparsity index calculation\n",
    "    max_possible_density = 1.0  # Considering the normalized image range\n",
    "    sparsity_index = (max_possible_density - rudy_pin_density) / max_possible_density\n",
    "\n",
    "    return {\"rudy_pin_sparsity_index\": sparsity_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [rudy_pin_clustering_coefficient, macro_connectivity_index, edge_congestion_ratio, macro_density_gradient, macro_distribution_evenness, rudy_pin_sparsity_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat_pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feat_func_list \u001b[38;5;241m=\u001b[39m [feat_func \u001b[38;5;28;01mfor\u001b[39;00m feat_func \u001b[38;5;129;01min\u001b[39;00m feat_func_list \u001b[38;5;28;01mif\u001b[39;00m feat_func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mfeat_pool\u001b[49m\u001b[38;5;241m.\u001b[39mkeys())]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feat_pool' is not defined"
     ]
    }
   ],
   "source": [
    "feat_func_list = [feat_func for feat_func in feat_func_list if feat_func.__name__ in list(feat_pool.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_func_list = feat_func_list + new_feat_func_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.rudy_gradient_variability(images)>,\n",
       " <function __main__.clustered_macro_distance_std(images)>,\n",
       " <function __main__.rudy_pin_clustering_coefficient(images)>,\n",
       " <function __main__.macro_connectivity_index(images)>,\n",
       " <function __main__.edge_congestion_ratio(images)>,\n",
       " <function __main__.macro_density_gradient(images)>,\n",
       " <function __main__.macro_distribution_evenness(images)>,\n",
       " <function __main__.rudy_pin_sparsity_index(images)>]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_func_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_feat_pool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m desc\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mnew_feat_pool\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_feat_pool' is not defined"
     ]
    }
   ],
   "source": [
    "desc.update(new_feat_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 3 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def rudy_pin_to_macro_density_variability(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro_image to [0, 255] and prepare binary image for contour detection\n",
    "    macro_image_uint8 = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image_uint8, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate densities (sum of pixel values) for each image within the unit square\n",
    "    macro_density = np.sum(macro_image) / total_image_area\n",
    "    rudy_density = np.sum(rudy_image) / total_image_area\n",
    "    rudy_pin_density = np.sum(rudy_pin_image) / total_image_area\n",
    "    \n",
    "    # Calculate variability as the standard deviation of densities across different regions\n",
    "    densities = np.array([macro_density, rudy_density, rudy_pin_density])\n",
    "    density_variability = np.std(densities)\n",
    "    \n",
    "    feature_value = density_variability * (tiles_size ** 2)  # Scale to μm²\n",
    "    \n",
    "    return {\"rudy_pin_to_macro_density_variability\": feature_value}\n",
    "\n",
    "\n",
    "def macro_aspect_ratio_variance(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    \n",
    "    # Convert the macro image from [0-1] to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold the macro image to create a binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of the macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate aspect ratio for each macro and store them\n",
    "    aspect_ratios = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        aspect_ratio = float(w) / float(h)\n",
    "        aspect_ratios.append(aspect_ratio)\n",
    "    \n",
    "    # Calculate the variance of aspect ratios\n",
    "    aspect_ratio_variance = np.var(aspect_ratios) * (tiles_size ** 2)\n",
    "    \n",
    "    # Return the feature\n",
    "    return {\"macro_aspect_ratio_variance\": aspect_ratio_variance}\n",
    "\n",
    "\n",
    "def gradient_alignment_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to 0-255 scale and create binary image\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Detect contours of macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Compute gradients in the RUDY image\n",
    "    grad_x = cv2.Sobel(rudy_image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    grad_y = cv2.Sobel(rudy_image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    # Threshold gradient magnitude to find areas of steep gradient\n",
    "    _, strong_gradients = cv2.threshold(gradient_magnitude, np.max(gradient_magnitude) * 0.5, 1, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find edges in the macro image\n",
    "    edges = cv2.Canny(np.uint8(macro_image), 100, 200)\n",
    "    \n",
    "    # Calculate alignment index based on overlap of edges and strong gradients\n",
    "    overlap = cv2.bitwise_and(edges, np.uint8(strong_gradients * 255))\n",
    "    alignment_score = np.sum(overlap) / np.sum(edges)\n",
    "    \n",
    "    # Convert to a feature measure\n",
    "    feature_value = alignment_score * 100  # Scale for better interpretability\n",
    "    \n",
    "    return {\"gradient_alignment_index\": feature_value}\n",
    "\n",
    "def rudy_edge_gradient_magnitude(images):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to 0-255 range\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate gradients on the RUDY image\n",
    "    grad_x = cv2.Sobel(rudy_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(rudy_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    \n",
    "    # Calculate gradient magnitude\n",
    "    grad_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    # Normalize to 0-1 range\n",
    "    grad_magnitude = cv2.normalize(grad_magnitude, None, 0, 1, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Find edges using Canny\n",
    "    edges = cv2.Canny(np.uint8(rudy_image * 255), 100, 200)\n",
    "    \n",
    "    # Compute mean gradient magnitude along edges\n",
    "    edge_grad_magnitudes = grad_magnitude[edges > 0]\n",
    "    rudy_edge_gradient_magnitude = np.mean(edge_grad_magnitudes)\n",
    "    \n",
    "    # Convert length to um\n",
    "    rudy_edge_gradient_magnitude_um = rudy_edge_gradient_magnitude * tiles_size\n",
    "    \n",
    "    return {\"rudy_edge_gradient_magnitude\": rudy_edge_gradient_magnitude_um}\n",
    "\n",
    "def macro_alignment_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold the macro image to create a binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours in the macro image to identify macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate RUDY high-density regions (threshold RUDY image for high density)\n",
    "    rudy_threshold = 0.5  # this threshold can be adjusted based on what constitutes \"high\" density\n",
    "    high_density_rudy = rudy_image > rudy_threshold\n",
    "    \n",
    "    # Initialize alignment sum\n",
    "    alignment_sum = 0\n",
    "    \n",
    "    # Calculate the alignment index\n",
    "    for contour in contours:\n",
    "        mask = np.zeros(macro_image.shape, dtype=np.uint8)\n",
    "        cv2.drawContours(mask, [contour], -1, (255), thickness=cv2.FILLED)\n",
    "        \n",
    "        macro_area = np.sum(mask > 0)\n",
    "        overlap_area = np.sum((mask > 0) & high_density_rudy)\n",
    "        overlap_ratio = overlap_area / macro_area if macro_area > 0 else 0\n",
    "        \n",
    "        alignment_sum += overlap_ratio\n",
    "    \n",
    "    # Normalize by the number of macros\n",
    "    macro_alignment_index_value = alignment_sum / len(contours) if len(contours) > 0 else 0\n",
    "    \n",
    "    # Convert the length from pixels to micrometers\n",
    "    macro_alignment_index_um = macro_alignment_index_value * tiles_size\n",
    "\n",
    "    return {\"macro_alignment_index\": macro_alignment_index_um}\n",
    "\n",
    "\n",
    "from scipy.stats import skew\n",
    "\n",
    "def rudy_pin_distribution_skewness(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0-255] range\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate the RUDY pin distribution skewness\n",
    "    rudy_pin_distribution = rudy_pin_image.flatten()  # Flatten the RUDY pin image\n",
    "    rudy_pin_distribution_um = rudy_pin_distribution * tiles_size  # Convert to um\n",
    "    \n",
    "    # Calculate the skewness of the RUDY pin distribution\n",
    "    rudy_pin_distribution_skewness_value = skew(rudy_pin_distribution_um)\n",
    "    \n",
    "    return {\"rudy_pin_distribution_skewness\": rudy_pin_distribution_skewness_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [rudy_pin_to_macro_density_variability, macro_aspect_ratio_variance, gradient_alignment_index, rudy_edge_gradient_magnitude, macro_alignment_index, rudy_pin_distribution_skewness]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 4 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def macro_compactness_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "\n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height * tiles_size * tiles_size\n",
    "\n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "\n",
    "    # Threshold and find contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Calculate total macro area and perimeter\n",
    "    total_macro_area = 0\n",
    "    total_perimeter = 0\n",
    "    \n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        total_macro_area += area\n",
    "        total_perimeter += perimeter\n",
    "\n",
    "    # Convert to real-world units (um^2 for area)\n",
    "    total_macro_area_um2 = total_macro_area * (tiles_size ** 2)\n",
    "    \n",
    "    # Calculate compactness index\n",
    "    macro_compactness_index = (total_perimeter ** 2) / total_macro_area_um2 if total_macro_area_um2 else 0\n",
    "\n",
    "    return {\"macro_compactness_index\": macro_compactness_index}\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def macro_arrangement_entropy(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate area of each macro in um^2\n",
    "    macro_areas = np.array([cv2.contourArea(c) * tiles_size * tiles_size for c in contours])\n",
    "    \n",
    "    # Normalize areas to probabilities\n",
    "    if len(macro_areas) > 0:\n",
    "        probabilities = macro_areas / macro_areas.sum()\n",
    "    else:\n",
    "        probabilities = np.array([])\n",
    "    \n",
    "    # Calculate entropy\n",
    "    feature_value = entropy(probabilities)\n",
    "    \n",
    "    return {\"macro_arrangement_entropy\": feature_value}\n",
    "\n",
    "\n",
    "def macro_edge_alignment_coefficient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    \n",
    "    # Convert macro_image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Find contours of macros\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate the macro edge alignment coefficient\n",
    "    alignment_score = 0\n",
    "    total_edge_length = 0\n",
    "    \n",
    "    # Iterate through each macro contour\n",
    "    for contour in contours:\n",
    "        # Approximate contour for a simpler shape (rectangle)\n",
    "        epsilon = 0.01 * cv2.arcLength(contour, True)\n",
    "        approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "        \n",
    "        for i in range(len(approx)):\n",
    "            # Calculate length of each edge\n",
    "            p1 = approx[i][0]\n",
    "            p2 = approx[(i + 1) % len(approx)][0]\n",
    "            edge_length = np.linalg.norm(p1 - p2) * tiles_size\n",
    "            \n",
    "            # Determine edge alignment using the rudy_image\n",
    "            # Calculate midpoint\n",
    "            midpoint = ((p1 + p2) / 2).astype(int)\n",
    "            rudy_value = rudy_image[midpoint[1], midpoint[0]]\n",
    "            \n",
    "            # Accumulate score based on edge alignment with rudy_value\n",
    "            alignment_score += edge_length * rudy_value\n",
    "            total_edge_length += edge_length\n",
    "            \n",
    "    if total_edge_length > 0:\n",
    "        macro_edge_alignment_coefficient = alignment_score / total_edge_length\n",
    "    else:\n",
    "        macro_edge_alignment_coefficient = 0\n",
    "\n",
    "    return {\"macro_edge_alignment_coefficient\": macro_edge_alignment_coefficient}\n",
    "\n",
    "def rudy_pin_gradient_directionality(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate the gradient of the rudy_pin_image\n",
    "    rudy_pin_dx = cv2.Sobel(rudy_pin_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    rudy_pin_dy = cv2.Sobel(rudy_pin_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "    # Compute the magnitude and angle of the gradients\n",
    "    magnitude = cv2.magnitude(rudy_pin_dx, rudy_pin_dy)\n",
    "    angle = cv2.phase(rudy_pin_dx, rudy_pin_dy, angleInDegrees=True)\n",
    "\n",
    "    # Calculate the histogram of gradient angles\n",
    "    angle_hist, _ = np.histogram(angle, bins=36, range=(0, 360))\n",
    "\n",
    "    # Find the predominant direction\n",
    "    predominant_direction = np.argmax(angle_hist)\n",
    "    feature_value = predominant_direction * 10  # Convert bin index to angle in degrees\n",
    "\n",
    "    return {\"rudy_pin_gradient_directionality\": feature_value}\n",
    "\n",
    "\n",
    "def rudy_pin_intensity_variation(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate intensity variation in RUDY pins\n",
    "    min_intensity = np.min(rudy_pin_image)\n",
    "    max_intensity = np.max(rudy_pin_image)\n",
    "    intensity_variation = max_intensity - min_intensity\n",
    "    \n",
    "    # Convert to physical units (um, considering each pixel is 2.25um x 2.25um)\n",
    "    intensity_variation_um = intensity_variation * tiles_size\n",
    "    \n",
    "    return {\"rudy_pin_intensity_variation\": intensity_variation_um}\n",
    "\n",
    "\n",
    "def macro_gap_ratio(images):\n",
    "    tiles_size = 2.25  # size of each tile in um\n",
    "    macro_image = images[0]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area_pixels = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0, 255]\n",
    "    macro_image_uint8 = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image_uint8, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Compute the area occupied by the macros\n",
    "    macro_area_pixels = sum(cv2.contourArea(contour) for contour in contours)\n",
    "    \n",
    "    # Calculate gaps area\n",
    "    gap_area_pixels = total_image_area_pixels - macro_area_pixels\n",
    "\n",
    "    # Convert pixel area to um^2\n",
    "    pixel_area_to_um2 = tiles_size * tiles_size\n",
    "    total_area_um2 = total_image_area_pixels * pixel_area_to_um2\n",
    "    gap_area_um2 = gap_area_pixels * pixel_area_to_um2\n",
    "    \n",
    "    # Calculate macro gap ratio\n",
    "    macro_gap_ratio_value = gap_area_um2 / total_area_um2\n",
    "    \n",
    "    return {\"macro_gap_ratio\": macro_gap_ratio_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [macro_compactness_index, macro_arrangement_entropy, macro_edge_alignment_coefficient, rudy_pin_gradient_directionality, rudy_pin_intensity_variation, macro_gap_ratio]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 5 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def rudy_pin_compaction_ratio(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to [0-255] grayscale\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Calculate total number of pixels in the image\n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "\n",
    "    # Determine number of macro blocks - not needed for this calculation\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "\n",
    "    # Process rudy pin image to find clusters\n",
    "    _, binary_rudy_pin = cv2.threshold(rudy_pin_image, 0.5, 1, cv2.THRESH_BINARY)\n",
    "    binary_rudy_pin = np.uint8(binary_rudy_pin * 255)\n",
    "\n",
    "    # Find contours in the binary Rudy pin image\n",
    "    contours, _ = cv2.findContours(binary_rudy_pin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate pin area and compacted pin area\n",
    "    total_pin_area = cv2.countNonZero(binary_rudy_pin) * (tiles_size ** 2)\n",
    "    num_clusters = len(contours)\n",
    "    \n",
    "    # Compute the compacted pin area \n",
    "    compacted_pin_area = sum(cv2.contourArea(c) for c in contours) * (tiles_size ** 2)\n",
    "\n",
    "    # Calculate the compaction ratio\n",
    "    rudy_pin_compaction_ratio = compacted_pin_area / total_pin_area if total_pin_area > 0 else 0\n",
    "\n",
    "    return {\"rudy_pin_compaction_ratio\": rudy_pin_compaction_ratio}\n",
    "\n",
    "\n",
    "def mean_macro_corner_distance(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to a [0-255] scale for contour detection\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Find macro contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Create binary mask of high-density RUDY regions (assuming threshold > 0.5 defines \"high-density\")\n",
    "    high_density_threshold = 0.5\n",
    "    rudy_binary = np.where(rudy_image > high_density_threshold, 1, 0).astype(np.uint8)\n",
    "    \n",
    "    # Find distances from each macro corner to the nearest high-density region\n",
    "    mean_distances = []\n",
    "    for contour in contours:\n",
    "        for point in contour:\n",
    "            corner_x, corner_y = point[0]\n",
    "            # Calculate distances to all high-density points\n",
    "            distances = cv2.distanceTransform(1 - rudy_binary, cv2.DIST_L2, 3)\n",
    "            distance_to_nearest = distances[corner_y, corner_x]\n",
    "            mean_distances.append(distance_to_nearest)\n",
    "    \n",
    "    # Convert pixel distances to micrometers\n",
    "    mean_distances_um = [dist * tiles_size for dist in mean_distances]\n",
    "    \n",
    "    # Calculate mean distance\n",
    "    if mean_distances_um:\n",
    "        feature_value = np.mean(mean_distances_um)\n",
    "    else:\n",
    "        feature_value = 0  # No macros or no high-density regions\n",
    "    \n",
    "    return {\"mean_macro_corner_distance\": feature_value}\n",
    "\n",
    "def rudy_intensity_contrast_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to a binary image to find macros\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate the RUDY intensity contrast index\n",
    "    rudy_contrast = 0.0\n",
    "    for y in range(image_height - 1):\n",
    "        for x in range(image_width - 1):\n",
    "            # Calculate the local contrast by comparing adjacent pixels\n",
    "            local_contrast_horizontal = abs(rudy_image[y, x] - rudy_image[y, x + 1])\n",
    "            local_contrast_vertical = abs(rudy_image[y, x] - rudy_image[y + 1, x])\n",
    "            \n",
    "            # Accumulate the contrast values\n",
    "            rudy_contrast += local_contrast_horizontal + local_contrast_vertical\n",
    "    \n",
    "    # Normalize the contrast value by the number of comparisons\n",
    "    num_contrasts = 2 * (image_height * (image_width - 1)) + 2 * (image_width * (image_height - 1))\n",
    "    rudy_intensity_contrast_index = rudy_contrast / num_contrasts\n",
    "    \n",
    "    return {\"rudy_intensity_contrast_index\": rudy_intensity_contrast_index}\n",
    "\n",
    "\n",
    "def macro_center_proximity_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to 0-255\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold to create binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate macro center proximity index\n",
    "    center_proximity_sum = 0\n",
    "    for contour in contours:\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] != 0:\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "\n",
    "            # Convert center coordinates to micrometers\n",
    "            cX_um = cX * tiles_size\n",
    "            cY_um = cY * tiles_size\n",
    "            \n",
    "            # Extract local RUDY value at the macro center\n",
    "            rudy_value_at_center = rudy_image[cY, cX]\n",
    "            \n",
    "            # Accumulate the proximity index contribution from each macro\n",
    "            center_proximity_sum += rudy_value_at_center\n",
    "    \n",
    "    # Average proximity index per macro\n",
    "    if num_macros > 0:\n",
    "        feature_value = center_proximity_sum / num_macros\n",
    "    else:\n",
    "        feature_value = 0\n",
    "    \n",
    "    return {\"macro_center_proximity_index\": feature_value}\n",
    "\n",
    "def angular_macro_concentration_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    \n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "\n",
    "    if num_macros < 2:\n",
    "        return {\"angular_macro_concentration_index\": 0}\n",
    "    \n",
    "    angles = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        if len(contour) >= 5:\n",
    "            ellipse = cv2.fitEllipse(contour)\n",
    "            angles.append(ellipse[-1])\n",
    "\n",
    "    angles = np.array(angles)\n",
    "    \n",
    "    mean_angle = np.mean(angles)\n",
    "    angular_diffs = (angles - mean_angle + 180) % 360 - 180\n",
    "    concentration_index = np.sum(angular_diffs**2) / num_macros\n",
    "    \n",
    "    feature_value = concentration_index\n",
    "    \n",
    "    return {\"angular_macro_concentration_index\": feature_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [rudy_pin_compaction_ratio, mean_macro_corner_distance, rudy_intensity_contrast_index, macro_center_proximity_index, angular_macro_concentration_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 6 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def macro_rudy_correlation_coefficient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to [0, 255]\n",
    "    macro_image_255 = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Binarize the macro image to detect contours\n",
    "    _, binary_image = cv2.threshold(macro_image_255, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate the macro mask (binary mask of macro placements)\n",
    "    macro_mask = np.zeros_like(macro_image_255, dtype=np.float32)\n",
    "    cv2.drawContours(macro_mask, contours, -1, (1), thickness=cv2.FILLED)\n",
    "\n",
    "    # Calculate the correlation \n",
    "    macro_flat = macro_mask.flatten()\n",
    "    rudy_flat = rudy_image.flatten()\n",
    "\n",
    "    # Calculate correlation\n",
    "    correlation_coefficient = np.corrcoef(macro_flat, rudy_flat)[0, 1]\n",
    "    \n",
    "    return {\"macro_rudy_correlation_coefficient\": correlation_coefficient}\n",
    "\n",
    "\n",
    "def multi_scale_density_variance(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to [0-255] range\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Get the dimensions of the image\n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Binarize the macro image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Define scales for multi-scale analysis\n",
    "    scales = [2, 4, 8, 16]  # Multiple scales\n",
    "    density_variances = []\n",
    "\n",
    "    # Analyze the density at each scale\n",
    "    for scale in scales:\n",
    "        window_size = int(scale * 2.25 / tiles_size)\n",
    "        density_map = np.zeros((image_height // window_size, image_width // window_size))\n",
    "\n",
    "        for y in range(0, image_height, window_size):\n",
    "            for x in range(0, image_width, window_size):\n",
    "                window = rudy_image[y:y + window_size, x:x + window_size]\n",
    "                density_map[y // window_size, x // window_size] = np.mean(window)\n",
    "\n",
    "        variance = np.var(density_map)\n",
    "        density_variances.append(variance)\n",
    "\n",
    "    # Compute the overall variance across all scales\n",
    "    feature_value = np.mean(density_variances)\n",
    "\n",
    "    return {\"multi_scale_density_variance\": feature_value}\n",
    "\n",
    "\n",
    "def macro_variability_coefficient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate macro areas\n",
    "    macro_areas_um2 = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour) * (tiles_size**2)\n",
    "        macro_areas_um2.append(area)\n",
    "    \n",
    "    num_macros = len(macro_areas_um2)\n",
    "    \n",
    "    if num_macros > 0:\n",
    "        # Calculate average and standard deviation of macro areas\n",
    "        mean_area = np.mean(macro_areas_um2)\n",
    "        std_dev_area = np.std(macro_areas_um2)\n",
    "        \n",
    "        # Variability coefficient\n",
    "        variability_coefficient = std_dev_area / mean_area\n",
    "    else:\n",
    "        variability_coefficient = 0\n",
    "    \n",
    "    return {\"macro_variability_coefficient\": variability_coefficient}\n",
    "\n",
    "\n",
    "def pin_rudy_density_coupling(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold the macro image to find contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate the RUDY density and pin density\n",
    "    rudy_density = np.mean(rudy_image)\n",
    "    pin_rudy_density = np.mean(rudy_pin_image)\n",
    "    \n",
    "    # Calculate the coupling feature\n",
    "    if rudy_density == 0:\n",
    "        # Avoid division by zero\n",
    "        pin_rudy_density_coupling_value = 0\n",
    "    else:\n",
    "        pin_rudy_density_coupling_value = pin_rudy_density / rudy_density\n",
    "    \n",
    "    # Convert the pixel-based measurement to um\n",
    "    feature_value_um = pin_rudy_density_coupling_value\n",
    "    \n",
    "    return {\"pin_rudy_density_coupling\": feature_value_um}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [macro_rudy_correlation_coefficient, multi_scale_density_variance, macro_variability_coefficient, pin_rudy_density_coupling]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 7 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_edge_proximity_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "\n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "\n",
    "    # Create binary image for finding contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Get dimensions and total area\n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height * (tiles_size ** 2)\n",
    "    \n",
    "    macro_edge_proximity_sum = 0\n",
    "\n",
    "    # Loop through each contour for macros\n",
    "    for contour in contours:\n",
    "        # Create mask for the current macro\n",
    "        macro_mask = np.zeros_like(macro_image)\n",
    "        cv2.drawContours(macro_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "        # Dilate contour edges slightly for proximity measurement\n",
    "        edge_mask = cv2.dilate(macro_mask, np.ones((3, 3), np.uint8), iterations=1) - macro_mask\n",
    "\n",
    "        # Overlay edge mask with RUDY image\n",
    "        proximity_area = np.sum((rudy_image * edge_mask) > 0)\n",
    "        macro_edge_proximity_sum += proximity_area\n",
    "\n",
    "    # Calculate proximity index\n",
    "    macro_edge_proximity_index = (macro_edge_proximity_sum * (tiles_size ** 2)) / total_image_area\n",
    "\n",
    "    return {\"macro_edge_proximity_index\": macro_edge_proximity_index}\n",
    "\n",
    "\n",
    "def corner_congestion_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    corner_congestion_values = []\n",
    "\n",
    "    for contour in contours:\n",
    "        # Approximate contour to get corners\n",
    "        epsilon = 0.01 * cv2.arcLength(contour, True)\n",
    "        approx_corners = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "        for corner in approx_corners:\n",
    "            x, y = corner.ravel()\n",
    "            \n",
    "            # Define a small region around the corner to measure congestion\n",
    "            region_size = 5  # 5 pixels around the corner\n",
    "            x_start = max(0, x - region_size)\n",
    "            y_start = max(0, y - region_size)\n",
    "            x_end = min(image_width, x + region_size)\n",
    "            y_end = min(image_height, y + region_size)\n",
    "            \n",
    "            # Extract regions around the corner from rudy and rudy_pin images\n",
    "            rudy_region = rudy_image[y_start:y_end, x_start:x_end]\n",
    "            rudy_pin_region = rudy_pin_image[y_start:y_end, x_start:x_end]\n",
    "            \n",
    "            # Calculate congestion based on the mean intensity values\n",
    "            corner_rudy_congestion = np.mean(rudy_region)\n",
    "            corner_rudy_pin_congestion = np.mean(rudy_pin_region)\n",
    "            \n",
    "            # Combine congestion measures into a single value (e.g., sum or weighted sum)\n",
    "            corner_total_congestion = corner_rudy_congestion + corner_rudy_pin_congestion\n",
    "            \n",
    "            corner_congestion_values.append(corner_total_congestion)\n",
    "    \n",
    "    # Calculate the corner congestion index as the average of corner congestion values\n",
    "    if len(corner_congestion_values) > 0:\n",
    "        corner_congestion_index_value = np.mean(corner_congestion_values)\n",
    "    else:\n",
    "        corner_congestion_index_value = 0\n",
    "\n",
    "    return {\"corner_congestion_index\": corner_congestion_index_value}\n",
    "\n",
    "\n",
    "def inverse_macro_density_gradient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "\n",
    "    # Threshold the macro image to get binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours to identify macros on the layout\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate macro density\n",
    "    macro_areas = [cv2.contourArea(cnt) for cnt in contours]\n",
    "    total_macro_area = sum(macro_areas)\n",
    "    \n",
    "    macro_density = total_macro_area / total_image_area\n",
    "    \n",
    "    # Calculate density gradient across the layout\n",
    "    sobelx = cv2.Sobel(binary_image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobely = cv2.Sobel(binary_image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    \n",
    "    gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    density_gradient = np.mean(gradient_magnitude)\n",
    "    \n",
    "    # Calculate inverse density gradient\n",
    "    inverse_density_gradient = 0 if density_gradient == 0 else 1 / density_gradient\n",
    "\n",
    "    # Convert gradient magnitude to um units\n",
    "    inverse_density_gradient_um = inverse_density_gradient * tiles_size\n",
    "    \n",
    "    return {\"inverse_macro_density_gradient\": inverse_density_gradient_um}\n",
    "\n",
    "\n",
    "def macro_symmetry_coefficient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0, 255] and threshold it\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Split the image into left and right halves\n",
    "    left_half = binary_image[:, :image_width // 2]\n",
    "    right_half = binary_image[:, image_width // 2:]\n",
    "    \n",
    "    # Calculate areas of macros on each half\n",
    "    left_area = cv2.countNonZero(left_half)\n",
    "    right_area = cv2.countNonZero(right_half)\n",
    "    \n",
    "    # Convert pixel areas to micrometers\n",
    "    left_area_um = left_area * tiles_size * tiles_size\n",
    "    right_area_um = right_area * tiles_size * tiles_size\n",
    "    \n",
    "    # Compute symmetry coefficient\n",
    "    symmetry_coefficient = abs(left_area_um - right_area_um) / (left_area_um + right_area_um)\n",
    "    \n",
    "    return {\"macro_symmetry_coefficient\": symmetry_coefficient}\n",
    "\n",
    "\n",
    "\n",
    "def macro_cluster_density_contrast(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate macro coverage area\n",
    "    macro_area = cv2.countNonZero(binary_image)\n",
    "    macro_area_um = macro_area * tiles_size ** 2\n",
    "    \n",
    "    # Calculate average RUDY in macro areas\n",
    "    rudy_macro_values = rudy_image[binary_image > 0]\n",
    "    average_rudy_macro = np.mean(rudy_macro_values)\n",
    "    \n",
    "    # Calculate overall average RUDY\n",
    "    average_rudy_total = np.mean(rudy_image)\n",
    "    \n",
    "    # Calculate density contrast\n",
    "    density_contrast = (average_rudy_macro - average_rudy_total) / average_rudy_total\n",
    "    \n",
    "    feature_value = density_contrast\n",
    "    \n",
    "    return {\"macro_cluster_density_contrast\": feature_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [macro_edge_proximity_index, corner_congestion_index, inverse_macro_density_gradient, macro_symmetry_coefficient, macro_cluster_density_contrast]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 8 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pin_macro_alignment_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro_image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "\n",
    "    # Threshold to create a binary image of macros\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours to identify macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "\n",
    "    # Identify the location of macros\n",
    "    macro_locations = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "    cv2.drawContours(macro_locations, contours, -1, 1, thickness=cv2.FILLED)\n",
    "\n",
    "    # Identify the pin locations above a certain threshold in rudy_pin_image\n",
    "    pin_threshold = 0.5  # Example threshold\n",
    "    pin_locations = np.where(rudy_pin_image > pin_threshold, 1, 0)\n",
    "\n",
    "    # Compute overlap between macro locations and pin locations\n",
    "    alignment_overlap = macro_locations * pin_locations\n",
    "\n",
    "    # Calculate the alignment index: ratio of aligned pins to total pins\n",
    "    total_pins = np.sum(pin_locations)\n",
    "    aligned_pins = np.sum(alignment_overlap)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    if total_pins == 0:\n",
    "        alignment_index = 0\n",
    "    else:\n",
    "        alignment_index = aligned_pins / total_pins\n",
    "\n",
    "    return {\"pin_macro_alignment_index\": alignment_index}\n",
    "\n",
    "\n",
    "def macro_density_uniformity_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to 8-bit\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold the image to create a binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of the macro regions\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Count the total pixels covered by macros\n",
    "    macro_area_pixels = sum(cv2.contourArea(c) for c in contours)\n",
    "    \n",
    "    # Convert macro area in pixels to area in um^2\n",
    "    macro_area_um2 = macro_area_pixels * tiles_size * tiles_size\n",
    "    \n",
    "    # Calculate expected area per macro if distributed uniformly\n",
    "    expected_macro_area = macro_area_um2 / num_macros if num_macros else 0\n",
    "    \n",
    "    # Calculate the variance of macro areas\n",
    "    areas = [cv2.contourArea(c) * tiles_size * tiles_size for c in contours]\n",
    "    variance = np.var(areas) if num_macros else 0\n",
    "    \n",
    "    # Define uniformity index as inverse variance (could be normalized)\n",
    "    macro_density_uniformity_index = 1 / (variance + 1e-5)  # Avoid division by zero\n",
    "    \n",
    "    return {\"macro_density_uniformity_index\": macro_density_uniformity_index}\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "def rudy_pin_distribution_kurtosis(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "\n",
    "    # Scale macro_image to [0, 255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold macro image to find contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "\n",
    "    # Calculate RUDY pin distribution kurtosis\n",
    "    rudy_pin_flat = rudy_pin_image.flatten()\n",
    "    rudy_pin_distribution_kurtosis_value = kurtosis(rudy_pin_flat)\n",
    "\n",
    "    return {\"rudy_pin_distribution_kurtosis\": rudy_pin_distribution_kurtosis_value}\n",
    "\n",
    "\n",
    "def localized_rudy_variability_coefficient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Scale macro image to [0, 255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Tile size in pixels\n",
    "    tile_size_px = int(256 / 16)\n",
    "    \n",
    "    rudy_variability = []\n",
    "    \n",
    "    # Iterate over tiles\n",
    "    for y in range(0, image_height, tile_size_px):\n",
    "        for x in range(0, image_width, tile_size_px):\n",
    "            # Extract the tile from the RUDY image\n",
    "            tile = rudy_image[y:y + tile_size_px, x:x + tile_size_px]\n",
    "            \n",
    "            if tile.size > 0:\n",
    "                # Flatten the tile to compute statistics\n",
    "                tile_values = tile.flatten()\n",
    "                # Calculate mean and standard deviation\n",
    "                mean_intensity = np.mean(tile_values)\n",
    "                std_dev_intensity = np.std(tile_values)\n",
    "                \n",
    "                if mean_intensity != 0:\n",
    "                    # Coefficient of Variation (CV) = std_dev / mean\n",
    "                    cv = std_dev_intensity / mean_intensity\n",
    "                else:\n",
    "                    cv = 0\n",
    "                \n",
    "                rudy_variability.append(cv)\n",
    "    \n",
    "    # Calculate the overall variability coefficient as the mean of CVs\n",
    "    if rudy_variability:\n",
    "        overall_variability_coefficient = np.mean(rudy_variability)\n",
    "    else:\n",
    "        overall_variability_coefficient = 0\n",
    "\n",
    "    return {\"localized_rudy_variability_coefficient\": overall_variability_coefficient}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [pin_macro_alignment_index, macro_density_uniformity_index, rudy_pin_distribution_kurtosis, localized_rudy_variability_coefficient]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 9 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def macro_distribution_clarity_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "\n",
    "    # Calculate total image area in um^2\n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area_um2 = (image_width * tiles_size) * (image_height * tiles_size)\n",
    "\n",
    "    # Threshold the macro image to create a binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate macro area\n",
    "    macro_area_pixels = sum(cv2.contourArea(contour) for contour in contours)\n",
    "    macro_area_um2 = macro_area_pixels * (tiles_size ** 2)\n",
    "\n",
    "    # Calculate the bounding area of the RUDY areas (define contrast)\n",
    "    rudy_contrast = cv2.Laplacian(rudy_image, cv2.CV_64F).var()\n",
    "    \n",
    "    # Combine contrast info with macro area clarity\n",
    "    clarity_index = (rudy_contrast + len(contours)) / (macro_area_um2 / total_image_area_um2)\n",
    "    \n",
    "    return {\"macro_distribution_clarity_index\": clarity_index}\n",
    "\n",
    "def rudy_variability_across_layers(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to 0-255 range\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate standard deviation for the RUDY image as a proxy for variability\n",
    "    rudy_variability = np.std(rudy_image)\n",
    "    \n",
    "    return {\"rudy_variability_across_layers\": rudy_variability}\n",
    "\n",
    "\n",
    "def macro_cross_layer_interaction_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height * (tiles_size ** 2)  # In um^2\n",
    "    \n",
    "    # Convert macro image to [0, 255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold the image to binary\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of the macro regions\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate interaction index\n",
    "    interaction_area = 0\n",
    "    for contour in contours:\n",
    "        # Create a mask for the current macro\n",
    "        macro_mask = np.zeros_like(macro_image)\n",
    "        cv2.drawContours(macro_mask, [contour], -1, (255), thickness=cv2.FILLED)\n",
    "        \n",
    "        # Find overlapping with RUDY and RUDY pin images\n",
    "        overlap_rudy = cv2.bitwise_and(rudy_image, rudy_image, mask=macro_mask)\n",
    "        overlap_rudy_pin = cv2.bitwise_and(rudy_pin_image, rudy_pin_image, mask=macro_mask)\n",
    "\n",
    "        # Calculate the interaction area in um^2\n",
    "        overlap_rudy_area = np.sum(overlap_rudy) * (tiles_size ** 2)\n",
    "        overlap_rudy_pin_area = np.sum(overlap_rudy_pin) * (tiles_size ** 2)\n",
    "\n",
    "        # Sum up the interaction areas\n",
    "        interaction_area += (overlap_rudy_area + overlap_rudy_pin_area)\n",
    "    \n",
    "    # Compute the macro cross-layer interaction index\n",
    "    macro_cross_layer_interaction_index = interaction_area / total_image_area\n",
    "    \n",
    "    return {\"macro_cross_layer_interaction_index\": macro_cross_layer_interaction_index}\n",
    "\n",
    "def rudy_pin_interaction_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height * (tiles_size**2)  # in um²\n",
    "    \n",
    "    # Convert macro_image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate interaction index\n",
    "    interaction_count = 0\n",
    "    for contour in contours:\n",
    "        # Draw contour on an empty black image\n",
    "        contour_image = np.zeros_like(rudy_pin_image)\n",
    "        cv2.drawContours(contour_image, [contour], -1, (255), thickness=cv2.FILLED)\n",
    "        \n",
    "        # Calculate intersection with rudy_pin_image\n",
    "        interaction_area = np.sum((contour_image / 255) * rudy_pin_image)\n",
    "        interaction_count += interaction_area\n",
    "    \n",
    "    # Convert area to um^2\n",
    "    interaction_count_um2 = interaction_count * (tiles_size**2)\n",
    "    \n",
    "    # Define interaction index as a ratio of interaction area over total layout area\n",
    "    rudy_pin_interaction_index_value = interaction_count_um2 / total_image_area\n",
    "    \n",
    "    return {\"rudy_pin_interaction_index\": rudy_pin_interaction_index_value}\n",
    "\n",
    "\n",
    "def macro_transition_slope_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold and find contours for macro regions\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate slope index\n",
    "    gradient_values = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        macro_region = rudy_image[y:y+h, x:x+w]  # Extract corresponding RUDY region\n",
    "        if macro_region.size > 0:\n",
    "            # Compute gradient (x and y direction)\n",
    "            grad_x = cv2.Sobel(macro_region, cv2.CV_64F, 1, 0, ksize=3)\n",
    "            grad_y = cv2.Sobel(macro_region, cv2.CV_64F, 0, 1, ksize=3)\n",
    "            \n",
    "            # Compute magnitude of gradient\n",
    "            gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "            \n",
    "            # Calculate average gradient magnitude for the macro region\n",
    "            avg_gradient_magnitude = np.mean(gradient_magnitude)\n",
    "            gradient_values.append(avg_gradient_magnitude)\n",
    "    \n",
    "    # Calculate the slope index by averaging the gradient values\n",
    "    macro_transition_slope_index = np.mean(gradient_values) if gradient_values else 0\n",
    "    \n",
    "    return {\"macro_transition_slope_index\": macro_transition_slope_index}\n",
    "\n",
    "\n",
    "def macro_to_edge_proximity_ratio(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    \n",
    "    # Convert macro image to [0, 255] scale for contour detection\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "\n",
    "    # Threshold to get binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours that represent macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # List to store distances of each macro to the nearest edge\n",
    "    distances = []\n",
    "    \n",
    "    # Dimensions of the image\n",
    "    image_height, image_width = macro_image.shape\n",
    "    \n",
    "    for contour in contours:\n",
    "        # Calculate bounding box of the contour\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Find distance to each edge\n",
    "        distance_left = x\n",
    "        distance_right = image_width - (x + w)\n",
    "        distance_top = y\n",
    "        distance_bottom = image_height - (y + h)\n",
    "        \n",
    "        # Find the minimum edge distance for this macro\n",
    "        min_distance = min(distance_left, distance_right, distance_top, distance_bottom)\n",
    "        \n",
    "        # Convert pixel distance to micrometers\n",
    "        min_distance_um = min_distance * tiles_size\n",
    "        distances.append(min_distance_um)\n",
    "    \n",
    "    # Compute the average distance\n",
    "    if len(distances) > 0:\n",
    "        average_distance_um = np.mean(distances)\n",
    "    else:\n",
    "        average_distance_um = 0\n",
    "\n",
    "    # Calculate the feature value\n",
    "    feature_value = average_distance_um\n",
    "    \n",
    "    return {\"macro_to_edge_proximity_ratio\": feature_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [macro_distribution_clarity_index, rudy_variability_across_layers, macro_cross_layer_interaction_index, rudy_pin_interaction_index, macro_transition_slope_index, macro_to_edge_proximity_ratio]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 10 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def rudy_direction_consistency_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to uint8\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Gradient calculation for RUDY image\n",
    "    grad_x = cv2.Sobel(rudy_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(rudy_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "    # Normalize gradients\n",
    "    magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
    "    angle = np.arctan2(grad_y, grad_x)\n",
    "\n",
    "    # Filter out zero magnitudes to avoid division by zero\n",
    "    valid_magnitudes = magnitude > 0\n",
    "    angle_filtered = angle[valid_magnitudes]\n",
    "\n",
    "    # Direction consistency calculation\n",
    "    # Calculate the variance of angles (directional consistency)\n",
    "    if angle_filtered.size > 0:\n",
    "        consistency_index = 1 - (np.var(angle_filtered) / (2 * np.pi))\n",
    "    else:\n",
    "        consistency_index = 0\n",
    "\n",
    "    return {\"rudy_direction_consistency_index\": consistency_index}\n",
    "\n",
    "\n",
    "def macro_routing_path_diversion(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = np.uint8(images[0] * 255)  # Convert to [0, 255]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Get image dimensions\n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Threshold to create binary image for macros\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of macro regions\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Create a mask for macro regions\n",
    "    macro_mask = np.zeros_like(rudy_image, dtype=np.uint8)\n",
    "    cv2.drawContours(macro_mask, contours, -1, 1, thickness=cv2.FILLED)\n",
    "    \n",
    "    # Calculate RUDY intensity inside and outside macros\n",
    "    rudy_inside_macro = rudy_image[macro_mask == 1].sum()\n",
    "    rudy_outside_macro = rudy_image[macro_mask == 0].sum()\n",
    "    \n",
    "    # Normalize based on the areas\n",
    "    area_inside = np.count_nonzero(macro_mask)\n",
    "    area_outside = total_image_area - area_inside\n",
    "    \n",
    "    # Calculate diversion index\n",
    "    # Diversion means higher routing inside macros compared to outside\n",
    "    if area_inside > 0 and area_outside > 0:\n",
    "        rudy_inside_macro_normalized = rudy_inside_macro / area_inside\n",
    "        rudy_outside_macro_normalized = rudy_outside_macro / area_outside\n",
    "        macro_routing_path_diversion = rudy_inside_macro_normalized - rudy_outside_macro_normalized\n",
    "    else:\n",
    "        macro_routing_path_diversion = 0\n",
    "\n",
    "    # Convert to specific units if necessary, considering each pixel is 2.25um x 2.25um\n",
    "    macro_routing_path_diversion *= (tiles_size ** 2)\n",
    "    \n",
    "    return {\"macro_routing_path_diversion\": macro_routing_path_diversion}\n",
    "\n",
    "\n",
    "def rudy_pin_alignment_uniformity(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate the uniformity of RUDY pins alignment\n",
    "    # Convert RUDY pin image to binary using a threshold\n",
    "    _, rudy_pin_binary = cv2.threshold(rudy_pin_image, 0.5, 1, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Sum along the main routing directions (vertical and horizontal)\n",
    "    vertical_sum = np.sum(rudy_pin_binary, axis=0)\n",
    "    horizontal_sum = np.sum(rudy_pin_binary, axis=1)\n",
    "    \n",
    "    # Calculate uniformity by measuring variance in these sums\n",
    "    vertical_variance = np.var(vertical_sum)\n",
    "    horizontal_variance = np.var(horizontal_sum)\n",
    "    \n",
    "    # Combine the variances, lower variance indicates more uniform alignment\n",
    "    alignment_uniformity = 1 / (1 + vertical_variance + horizontal_variance)\n",
    "    \n",
    "    return {\"rudy_pin_alignment_uniformity\": alignment_uniformity}\n",
    "\n",
    "\n",
    "def macro_corner_density_contrast(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Binarize the macro image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # For each macro, calculate edge and center densities\n",
    "    macro_corner_contrasts = []\n",
    "    \n",
    "    for contour in contours:\n",
    "        # Get bounding box of the macro\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # Define regions: corners and center\n",
    "        corner_regions = [\n",
    "            (x, y, x + w//2, y + h//2),  # Top-left\n",
    "            (x + w//2, y, x + w, y + h//2),  # Top-right\n",
    "            (x, y + h//2, x + w//2, y + h),  # Bottom-left\n",
    "            (x + w//2, y + h//2, x + w, y + h)  # Bottom-right\n",
    "        ]\n",
    "        \n",
    "        center_region = (x + w//4, y + h//4, x + 3*w//4, y + 3*h//4)\n",
    "        \n",
    "        # Calculate densities\n",
    "        corner_density_sum = sum(\n",
    "            np.mean(rudy_image[y1:y2, x1:x2]) for (x1, y1, x2, y2) in corner_regions\n",
    "        )\n",
    "        center_density = np.mean(rudy_image[center_region[1]:center_region[3], center_region[0]:center_region[2]])\n",
    "        \n",
    "        # Calculate density contrast\n",
    "        density_contrast = corner_density_sum / len(corner_regions) - center_density\n",
    "        macro_corner_contrasts.append(density_contrast)\n",
    "    \n",
    "    # Calculate an average or other statistics if needed\n",
    "    feature_value = np.mean(macro_corner_contrasts)\n",
    "    \n",
    "    return {\"macro_corner_density_contrast\": feature_value}\n",
    "\n",
    "def rudy_pin_area_masking_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0, 255] and create a binary mask\n",
    "    macro_image_uint8 = np.uint8(macro_image * 255)\n",
    "    _, macro_mask = cv2.threshold(macro_image_uint8, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Calculate the area of the RUDY pin image that is masked by macros\n",
    "    rudy_pin_area = np.sum(rudy_pin_image)\n",
    "    masked_rudy_pin_area = np.sum(rudy_pin_image * (macro_mask / 255))\n",
    "    \n",
    "    # Calculate the rudy pin area masking index\n",
    "    if rudy_pin_area == 0:\n",
    "        rudy_pin_area_masking_index = 0\n",
    "    else:\n",
    "        rudy_pin_area_masking_index = masked_rudy_pin_area / rudy_pin_area\n",
    "    \n",
    "    # Convert pixels to area in um^2\n",
    "    total_image_area_um2 = total_image_area * (tiles_size ** 2)\n",
    "    rudy_pin_area_masked_um2 = rudy_pin_area_masking_index * total_image_area_um2\n",
    "    \n",
    "    return {\"rudy_pin_area_masking_index\": rudy_pin_area_masking_index}\n",
    "\n",
    "\n",
    "def macro_rudy_alignment_gradient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to 8-bit\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold to get binary macro image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours for macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate gradients in RUDY image\n",
    "    rudy_gradient_x = cv2.Sobel(rudy_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    rudy_gradient_y = cv2.Sobel(rudy_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    \n",
    "    # Initialize alignment gradient\n",
    "    alignment_gradient_sum = 0.0\n",
    "    \n",
    "    # Calculate alignment gradient for each macro\n",
    "    for contour in contours:\n",
    "        for point in contour:\n",
    "            x, y = point[0]\n",
    "            grad_x = rudy_gradient_x[y, x]\n",
    "            grad_y = rudy_gradient_y[y, x]\n",
    "            \n",
    "            alignment_gradient_sum += np.sqrt(grad_x**2 + grad_y**2)\n",
    "    \n",
    "    # Normalize by number of macros and convert pixel gradient to um gradient\n",
    "    macro_area_um2 = num_macros * tiles_size**2\n",
    "    alignment_gradient = alignment_gradient_sum / (macro_area_um2 if macro_area_um2 != 0 else 1)\n",
    "    \n",
    "    return {\"macro_rudy_alignment_gradient\": alignment_gradient}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [rudy_direction_consistency_index, macro_routing_path_diversion, rudy_pin_alignment_uniformity, macro_corner_density_contrast, rudy_pin_area_masking_index, macro_rudy_alignment_gradient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_rudy_alignment_gradient': np.float64(0.1510136903429559)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feat_func_list[5](image_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 11 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_centralization_index(images):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0, 255] scale\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold to find contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate image center\n",
    "    image_center = np.array([image_width / 2, image_height / 2])\n",
    "    \n",
    "    # Calculate centroids of macros\n",
    "    centroids = []\n",
    "    for cnt in contours:\n",
    "        M = cv2.moments(cnt)\n",
    "        if M['m00'] != 0:\n",
    "            cx = int(M['m10'] / M['m00'])\n",
    "            cy = int(M['m01'] / M['m00'])\n",
    "            centroids.append((cx, cy))\n",
    "    \n",
    "    if not centroids:\n",
    "        return {\"macro_centralization_index\": 0}\n",
    "    \n",
    "    # Calculate average distance of centroids to image center\n",
    "    distances = [np.linalg.norm(np.array(c) - image_center) for c in centroids]\n",
    "    avg_distance = np.mean(distances)\n",
    "    \n",
    "    # Normalize based on the maximum possible distance (corner to center)\n",
    "    max_distance = np.linalg.norm(np.array([0, 0]) - image_center)\n",
    "    centralization_index = 1 - (avg_distance / max_distance)\n",
    "    \n",
    "    return {\"macro_centralization_index\": centralization_index}\n",
    "\n",
    "def rudy_pin_gradient_convergence(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "\n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "\n",
    "    # Convert macro image to binary\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours in the macro image\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "\n",
    "    # Compute gradients of the RUDY pin image\n",
    "    grad_x = cv2.Sobel(rudy_pin_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(rudy_pin_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "    # Calculate the gradient magnitude and direction\n",
    "    magnitude = cv2.magnitude(grad_x, grad_y)\n",
    "    angle = cv2.phase(grad_x, grad_y)\n",
    "\n",
    "    # Normalize the magnitude\n",
    "    magnitude /= magnitude.max()\n",
    "\n",
    "    # Calculate the histogram of gradient directions\n",
    "    hist_bins = 36\n",
    "    hist_range = (0, 2 * np.pi)\n",
    "    hist, _ = np.histogram(angle, bins=hist_bins, range=hist_range, weights=magnitude)\n",
    "\n",
    "    # Normalize the histogram\n",
    "    hist /= hist.sum()\n",
    "\n",
    "    # Compute the convergence feature as the entropy of the distribution\n",
    "    epsilon = 1e-5  # small value to avoid log(0)\n",
    "    entropy = -np.sum(hist * np.log(hist + epsilon))\n",
    "\n",
    "    # Adjust the entropy to fit as a convergence metric\n",
    "    feature_value = 1.0 / (entropy + epsilon)\n",
    "\n",
    "    return {\"rudy_pin_gradient_convergence\": feature_value}\n",
    "\n",
    "\n",
    "def macro_distance_slope_variance(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to binary\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    centroids = []\n",
    "    for contour in contours:\n",
    "        M = cv2.moments(contour)\n",
    "        if M[\"m00\"] != 0:  # Avoid division by zero\n",
    "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "            centroids.append((cX, cY))\n",
    "    \n",
    "    num_centroids = len(centroids)\n",
    "    if num_centroids < 2:\n",
    "        return {\"macro_distance_slope_variance\": 0.0}  # If less than 2 centroids, variance is undefined.\n",
    "    \n",
    "    # Calculate pairwise distances\n",
    "    distances = []\n",
    "    for i in range(num_centroids):\n",
    "        for j in range(i + 1, num_centroids):\n",
    "            cX1, cY1 = centroids[i]\n",
    "            cX2, cY2 = centroids[j]\n",
    "            distance = np.sqrt((cX2 - cX1)**2 + (cY2 - cY1)**2)\n",
    "            distances.append(distance)\n",
    "    \n",
    "    # Calculate slopes between pairwise distances\n",
    "    slopes = []\n",
    "    for i in range(len(distances) - 1):\n",
    "        slope = distances[i + 1] - distances[i]\n",
    "        slopes.append(slope)\n",
    "    \n",
    "    # Calculate variance of slopes\n",
    "    if len(slopes) == 0:\n",
    "        variance = 0.0  # No slopes if there's only one distance\n",
    "    else:\n",
    "        variance = np.var(slopes) * (tiles_size**2)\n",
    "    \n",
    "    return {\"macro_distance_slope_variance\": variance}\n",
    "\n",
    "def macro_surface_roughness_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to [0-255] scale\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold and find contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate the perimeter and area of macros\n",
    "    total_perimeter = 0\n",
    "    total_area = 0\n",
    "    \n",
    "    for contour in contours:\n",
    "        # Calculate perimeter\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        total_perimeter += perimeter\n",
    "        \n",
    "        # Calculate area\n",
    "        area = cv2.contourArea(contour)\n",
    "        total_area += area\n",
    "    \n",
    "    # Calculate roughness index as perimeter to area ratio\n",
    "    # Convert area from pixels to microns\n",
    "    total_area_um2 = total_area * (tiles_size ** 2)\n",
    "    \n",
    "    # Roughness index: perimeter (in pixels) / area (in um^2)\n",
    "    roughness_index = total_perimeter / total_area_um2\n",
    "    \n",
    "    return {\"macro_surface_roughness_index\": roughness_index}\n",
    "\n",
    "def rudy_pin_dispersal_evenness(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0, 255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Analyze RUDY pin distribution\n",
    "    pin_density = np.sum(rudy_pin_image, axis=(0, 1)) / total_image_area\n",
    "    \n",
    "    # Divide the image into grids to get local densities\n",
    "    grid_size = 16  # Example size, can be adjusted\n",
    "    grid_area = (grid_size / tiles_size) ** 2\n",
    "    pin_densities = []\n",
    "    \n",
    "    for i in range(0, image_height, grid_size):\n",
    "        for j in range(0, image_width, grid_size):\n",
    "            grid = rudy_pin_image[i:i+grid_size, j:j+grid_size]\n",
    "            local_density = np.sum(grid) / grid_area\n",
    "            pin_densities.append(local_density)\n",
    "    \n",
    "    # Calculate variance of local densities\n",
    "    variance = np.var(pin_densities)\n",
    "    \n",
    "    # Calculate evenness as inverse of variance\n",
    "    if variance == 0:\n",
    "        evenness = 1.0  # Perfectly even\n",
    "    else:\n",
    "        evenness = 1 / variance\n",
    "    \n",
    "    return {\"rudy_pin_dispersal_evenness\": evenness}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [macro_centralization_index, rudy_pin_gradient_convergence, macro_distance_slope_variance, macro_surface_roughness_index, rudy_pin_dispersal_evenness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def average_macro_overlap_factor(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Transform macro image to [0-255] range\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "\n",
    "    # Create binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours of macros\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate areas\n",
    "    macro_areas = [cv2.contourArea(contour) for contour in contours]\n",
    "    total_macro_area = sum(macro_areas)\n",
    "    \n",
    "    # Calculate overlap\n",
    "    overlap_image = np.zeros_like(binary_image)\n",
    "    cv2.drawContours(overlap_image, contours, -1, 255, -1)\n",
    "    intersection = cv2.bitwise_and(overlap_image, binary_image)\n",
    "    overlap_area = np.count_nonzero(intersection)\n",
    "    \n",
    "    # Calculate overlap factor\n",
    "    overlap_factor = overlap_area / total_macro_area if total_macro_area > 0 else 0\n",
    "    \n",
    "    # Convert factor to micrometer square units\n",
    "    overlap_factor *= (tiles_size**2) / total_image_area\n",
    "    \n",
    "    return {\"average_macro_overlap_factor\": overlap_factor}\n",
    "\n",
    "\n",
    "def macro_alignment_deviation_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to uint8\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold and find contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate alignment deviation\n",
    "    total_deviation = 0\n",
    "\n",
    "    for contour in contours:\n",
    "        # Fit a minimum area rectangle to the contour\n",
    "        rect = cv2.minAreaRect(contour)\n",
    "        angle = rect[-1]  # Get the angle of the rectangle\n",
    "        \n",
    "        # Adjust angle range: ensure angle is within [-90, 90]\n",
    "        if angle < -45:\n",
    "            angle += 90\n",
    "        \n",
    "        # Calculate deviation from optimal alignment (0 degrees)\n",
    "        optimal_alignment_angle = 0\n",
    "        deviation = abs(angle - optimal_alignment_angle)\n",
    "        total_deviation += deviation\n",
    "    \n",
    "    # Normalize the deviation by the number of macros\n",
    "    if num_macros > 0:\n",
    "        average_deviation = total_deviation / num_macros\n",
    "    else:\n",
    "        average_deviation = 0\n",
    "\n",
    "    # Return the result\n",
    "    return {\"macro_alignment_deviation_index\": average_deviation}\n",
    "\n",
    "def macro_boundary_density_variance(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Binarize the image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours in the macro image\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate density along the perimeters of macros\n",
    "    densities = []\n",
    "    for contour in contours:\n",
    "        perimeter = cv2.arcLength(contour, True)\n",
    "        macro_area = cv2.contourArea(contour)\n",
    "        \n",
    "        # Calculate density: area per unit length of perimeter\n",
    "        # Since we're dealing with a 2D image, perimeter is a reasonable approximation for analyzing edge features\n",
    "        if perimeter > 0:\n",
    "            density = macro_area / perimeter\n",
    "            densities.append(density)\n",
    "    \n",
    "    # Calculate the variance of the densities\n",
    "    if densities:\n",
    "        density_variance = np.var(densities)\n",
    "    else:\n",
    "        density_variance = 0.0\n",
    "    \n",
    "    return {\"macro_boundary_density_variance\": density_variance}\n",
    "\n",
    "\n",
    "def rudy_intensity_symmetry_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate symmetry index for rudy_image\n",
    "    left_half = rudy_image[:, :image_width // 2]\n",
    "    right_half = rudy_image[:, image_width // 2:]\n",
    "    \n",
    "    # Flip right half horizontally\n",
    "    flipped_right_half = cv2.flip(right_half, 1)\n",
    "\n",
    "    # Compute the absolute difference between left half and flipped right half\n",
    "    diff_image = cv2.absdiff(left_half, flipped_right_half)\n",
    "\n",
    "    # Sum of differences as a measure of asymmetry\n",
    "    asymmetry_measure = np.sum(diff_image)\n",
    "\n",
    "    # Normalize the measure relative to the total possible maximal intensity difference\n",
    "    max_intensity = 1.0  # Since rudy_image is in range [0, 1]\n",
    "    max_possible_diff = max_intensity * (image_width // 2) * image_height\n",
    "    symmetry_index = 1 - (asymmetry_measure / max_possible_diff)\n",
    "    \n",
    "    return {\"rudy_intensity_symmetry_index\": symmetry_index}\n",
    "\n",
    "\n",
    "\n",
    "def macro_isolation_factor(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to 0-255 range\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Get image dimensions and total area\n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "\n",
    "    # Detect macros as contours in the macro_image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Initialization for isolation factor calculation\n",
    "    isolation_factors = []\n",
    "\n",
    "    # Iterate through each macro (contour)\n",
    "    for contour in contours:\n",
    "        # Calculate macro area\n",
    "        macro_area_pixels = cv2.contourArea(contour)\n",
    "        macro_area_um = macro_area_pixels * (tiles_size ** 2)\n",
    "        \n",
    "        # Create a mask for current macro and dilate to find surrounding area\n",
    "        macro_mask = np.zeros_like(macro_image)\n",
    "        cv2.drawContours(macro_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "        \n",
    "        # Dilate mask to include surrounding area\n",
    "        surrounding_area_mask = cv2.dilate(macro_mask, np.ones((3, 3), np.uint8), iterations=5)\n",
    "        surrounding_area_mask[macro_mask == 255] = 0  # Remove macro area from surrounding\n",
    "        \n",
    "        # Calculate average RUDY value in the surrounding area\n",
    "        surrounding_rudy_values = rudy_image[surrounding_area_mask == 255]\n",
    "        surrounding_rudy_average = np.mean(surrounding_rudy_values)\n",
    "        \n",
    "        # Calculate isolation factor as a ratio of macro area to surrounding RUDY density\n",
    "        if surrounding_rudy_average > 0:\n",
    "            isolation_factor = macro_area_um / surrounding_rudy_average\n",
    "        else:\n",
    "            isolation_factor = np.inf  # Treat as highly isolated if no surrounding RUDY\n",
    "\n",
    "        isolation_factors.append(isolation_factor)\n",
    "    \n",
    "    # Overall isolation factor (could be average or another statistical measure)\n",
    "    feature_value = np.mean(isolation_factors)\n",
    "    \n",
    "    return {\"macro_isolation_factor\": feature_value}\n",
    "\n",
    "\n",
    "\n",
    "def macro_overlap_gradient(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert the macro image to [0, 255] scale\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Convert to binary image\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours for the macro layout\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Calculate gradient of macro coverage\n",
    "    grad_x = cv2.Sobel(binary_image, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    grad_y = cv2.Sobel(binary_image, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    \n",
    "    # Calculate magnitude of gradient\n",
    "    gradient_magnitude = cv2.magnitude(grad_x, grad_y)\n",
    "    \n",
    "    # Calculate mean gradient magnitude as a measure of macro overlap gradient\n",
    "    macro_overlap_gradient_value = np.mean(gradient_magnitude)\n",
    "    \n",
    "    # Convert gradient to physical units\n",
    "    macro_overlap_gradient_um = macro_overlap_gradient_value * tiles_size\n",
    "    \n",
    "    return {\"macro_overlap_gradient\": macro_overlap_gradient_um}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [average_macro_overlap_factor, macro_alignment_deviation_index, macro_boundary_density_variance, rudy_intensity_symmetry_index, macro_isolation_factor, macro_overlap_gradient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'macro_boundary_density_variance': np.float64(0.0)}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feat_func_list[2](image_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 12 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def macro_occupation_ratio(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to 8-bit grayscale [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    center_area = 0\n",
    "    peripheral_area = 0\n",
    "    \n",
    "    # Define central region (e.g., central 50% x 50%)\n",
    "    center_x_min = image_width * 0.25\n",
    "    center_x_max = image_width * 0.75\n",
    "    center_y_min = image_height * 0.25\n",
    "    center_y_max = image_height * 0.75\n",
    "\n",
    "    for cnt in contours:\n",
    "        area = cv2.contourArea(cnt)\n",
    "        \n",
    "        # Calculate bounding box\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        \n",
    "        # Check if the bounding box is within the center region\n",
    "        if (center_x_min <= x < center_x_max) and (center_y_min <= y < center_y_max):\n",
    "            center_area += area\n",
    "        else:\n",
    "            peripheral_area += area\n",
    "    \n",
    "    # Calculate areas in um^2\n",
    "    center_area_um2 = center_area * (tiles_size ** 2)\n",
    "    peripheral_area_um2 = peripheral_area * (tiles_size ** 2)\n",
    "    \n",
    "    # Calculate ratio\n",
    "    if peripheral_area_um2 == 0:\n",
    "        feature_value = float('inf')  # Avoid division by zero\n",
    "    else:\n",
    "        feature_value = center_area_um2 / peripheral_area_um2\n",
    "\n",
    "    return {\"macro_occupation_ratio\": feature_value}\n",
    "\n",
    "def rudy_congestion_hotspot_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Calculate image dimensions and area\n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Threshold macro image to find contours\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate the RUDY congestion hotspot index\n",
    "    # Find hotspots in the RUDY map\n",
    "    # Define a threshold to consider a pixel as a congestion hotspot\n",
    "    threshold_value = 0.5  # This value can be adjusted based on specific needs\n",
    "    rudy_hotspots = (rudy_image > threshold_value).astype(np.float32)\n",
    "    \n",
    "    # Calculate the frequency and intensity of congestion hotspots\n",
    "    hotspot_intensity = np.sum(rudy_hotspots)\n",
    "    hotspot_frequency = np.count_nonzero(rudy_hotspots)\n",
    "    \n",
    "    # Normalize the index by total area\n",
    "    rudy_congestion_hotspot_index = (hotspot_intensity / hotspot_frequency) if hotspot_frequency > 0 else 0\n",
    "    \n",
    "    # Convert pixel frequency to area in um^2\n",
    "    rudy_congestion_hotspot_index_um2 = rudy_congestion_hotspot_index * (tiles_size ** 2)\n",
    "\n",
    "    return {\"rudy_congestion_hotspot_index\": rudy_congestion_hotspot_index_um2}\n",
    "\n",
    "\n",
    "def macro_dispersion_uniformity(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to 0-255 scale\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate centroids of contours\n",
    "    centroids = []\n",
    "    for cnt in contours:\n",
    "        M = cv2.moments(cnt)\n",
    "        if M['m00'] != 0:\n",
    "            cx = M['m10'] / M['m00']\n",
    "            cy = M['m01'] / M['m00']\n",
    "            centroids.append((cx, cy))\n",
    "    \n",
    "    # Compute dispersion uniformity\n",
    "    if centroids:\n",
    "        centroid_array = np.array(centroids)\n",
    "        mean_center = np.mean(centroid_array, axis=0)\n",
    "        variances = np.var(centroid_array - mean_center, axis=0)\n",
    "        uniformity = 1 / (1 + np.sum(variances))  # Higher value means more uniform\n",
    "    else:\n",
    "        uniformity = 0\n",
    "    \n",
    "    return {\"macro_dispersion_uniformity\": uniformity}\n",
    "\n",
    "\n",
    "def rudy_pin_gradient_intensity_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate gradients in the rudy pin image\n",
    "    sobelx = cv2.Sobel(rudy_pin_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(rudy_pin_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    gradient_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    gradient_magnitude = cv2.normalize(gradient_magnitude, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Calculate the rudy_pin_gradient_intensity_index\n",
    "    feature_value = np.mean(gradient_magnitude[gradient_magnitude > 0])\n",
    "    \n",
    "    # Convert the feature_value to um\n",
    "    feature_value_um = feature_value * total_image_area * (tiles_size**2)\n",
    "    \n",
    "    return {\"rudy_pin_gradient_intensity_index\": feature_value_um}\n",
    "\n",
    "\n",
    "def rudy_deviation_effect_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image from [0, 1] to [0, 255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate macro area in pixel units\n",
    "    macro_area_pixel = np.sum(binary_image == 255)\n",
    "    \n",
    "    # Calculate macro area in um^2\n",
    "    macro_area_um2 = macro_area_pixel * (tiles_size ** 2)\n",
    "    \n",
    "    # Calculate average RUDY intensity\n",
    "    avg_rudy_intensity = np.mean(rudy_image)\n",
    "    \n",
    "    # Calculate deviation of RUDY intensity\n",
    "    deviation_rudy_intensity = np.std(rudy_image)\n",
    "    \n",
    "    # Calculate a measure of how deviations in RUDY intensity correlate with macro density\n",
    "    # A simplified approach assuming positive correlation\n",
    "    correlation_measure = deviation_rudy_intensity * macro_area_um2 / total_image_area\n",
    "    \n",
    "    feature_value = correlation_measure\n",
    "    \n",
    "    return {\"rudy_deviation_effect_index\": feature_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [macro_occupation_ratio, rudy_congestion_hotspot_index, macro_dispersion_uniformity, rudy_pin_gradient_intensity_index, rudy_deviation_effect_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage 13 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def demarcated_macro_proximity_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    \n",
    "    # Threshold the macro image to find contours of the macros\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Threshold for defining high-density areas in RUDY map\n",
    "    rudy_threshold = 0.5\n",
    "    \n",
    "    # Identifying high-density zones in RUDY\n",
    "    high_density_zone = rudy_image > rudy_threshold\n",
    "    \n",
    "    # Initialize a variable to accumulate proximity measure\n",
    "    proximity_sum = 0\n",
    "    \n",
    "    # Calculate proximity index\n",
    "    for contour in contours:\n",
    "        macro_mask = np.zeros_like(macro_image)\n",
    "        cv2.drawContours(macro_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "        \n",
    "        intersection = np.logical_and(macro_mask > 0, high_density_zone)\n",
    "        intersection_area = np.sum(intersection) * tiles_size * tiles_size\n",
    "        \n",
    "        # Proximity for current macro\n",
    "        proximity_sum += intersection_area\n",
    "        \n",
    "    # Normalizing by total macro area\n",
    "    total_macro_area = np.sum(macro_image > 0) * tiles_size * tiles_size\n",
    "    demarcated_macro_proximity_index = (proximity_sum / (total_macro_area + 1e-5)) if total_macro_area > 0 else 0\n",
    "    \n",
    "    return {\"demarcated_macro_proximity_index\": demarcated_macro_proximity_index}\n",
    "\n",
    "\n",
    "def macro_surface_irregularity_index(images):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height * (tiles_size ** 2)  # Convert area to micrometers\n",
    "\n",
    "    macro_image = np.uint8(macro_image * 255)  # Convert macro image to [0-255]\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "\n",
    "    # Calculate total perimeter and area of macros\n",
    "    total_perimeter = 0\n",
    "    total_macro_area = 0\n",
    "    for contour in contours:\n",
    "        total_perimeter += cv2.arcLength(contour, True)\n",
    "        total_macro_area += cv2.contourArea(contour)\n",
    "\n",
    "    # Convert perimeter and area to micrometers\n",
    "    total_perimeter_um = total_perimeter * tiles_size\n",
    "    total_macro_area_um = total_macro_area * (tiles_size ** 2)\n",
    "\n",
    "    # Irregularity index calculation\n",
    "    if total_macro_area_um > 0:\n",
    "        irregularity_index = total_perimeter_um / total_macro_area_um\n",
    "    else:\n",
    "        irregularity_index = 0\n",
    "\n",
    "    return {\"macro_surface_irregularity_index\": irregularity_index}\n",
    "\n",
    "\n",
    "def macro_rudy_boundary_interaction_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Calculate interaction index\n",
    "    interaction_index = 0\n",
    "    \n",
    "    # Iterate over each contour\n",
    "    for contour in contours:\n",
    "        # Create a mask of the macro contour\n",
    "        contour_mask = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "        cv2.drawContours(contour_mask, [contour], -1, (1), thickness=cv2.FILLED)\n",
    "        \n",
    "        # Calculate the intersection of the macro contour with RUDY pin image\n",
    "        interaction_area = np.sum(contour_mask * rudy_pin_image)\n",
    "        \n",
    "        # Convert pixels to area in um^2 (each pixel is 2.25um x 2.25um)\n",
    "        interaction_area_um = interaction_area * (tiles_size ** 2)\n",
    "        \n",
    "        # Check if the averaged RUDY pin density along macro boundary indicates congestion\n",
    "        if interaction_area > 0:\n",
    "            interaction_index += interaction_area_um / cv2.arcLength(contour, True) * tiles_size\n",
    "\n",
    "    return {\"macro_rudy_boundary_interaction_index\": interaction_index}\n",
    "\n",
    "def pin_density_peak_contrast(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "\n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "\n",
    "    # Convert macro image to [0-255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "\n",
    "    # Compute pin density peaks\n",
    "    # Using a kernel to emphasize local density peaks\n",
    "    kernel_size = 3  # Define an appropriate kernel size\n",
    "    kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size**2)\n",
    "    smoothed_rudy_pin = cv2.filter2D(rudy_pin_image, -1, kernel)\n",
    "    \n",
    "    # Find pin density peak contrast\n",
    "    pin_density_peak = np.max(smoothed_rudy_pin)\n",
    "    avg_density_around_peaks = np.mean(smoothed_rudy_pin)\n",
    "    \n",
    "    # Compute the contrast\n",
    "    contrast = pin_density_peak - avg_density_around_peaks\n",
    "\n",
    "    # Convert image dimensions to a physical measurement\n",
    "    contrast_um = contrast * tiles_size  # Convert to um using the tile size\n",
    "    \n",
    "    return {\"pin_density_peak_contrast\": contrast_um}\n",
    "\n",
    "\n",
    "def rudy_pin_density_flux_index(images):\n",
    "    tiles_size = 2.25\n",
    "    macro_image = images[0]\n",
    "    rudy_image = images[1]\n",
    "    rudy_pin_image = images[2]\n",
    "    \n",
    "    image_height, image_width = macro_image.shape\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Convert macro image to [0, 255]\n",
    "    macro_image = np.uint8(macro_image * 255)\n",
    "    _, binary_image = cv2.threshold(macro_image, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    num_macros = len(contours)\n",
    "    \n",
    "    # Process the RUDY pin image\n",
    "    # Compute the gradients along the x and y axis\n",
    "    grad_x = cv2.Sobel(rudy_pin_image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    grad_y = cv2.Sobel(rudy_pin_image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    \n",
    "    # Compute the gradient magnitude\n",
    "    grad_magnitude = cv2.magnitude(grad_x, grad_y)\n",
    "    \n",
    "    # Calculate the mean of the gradient magnitude to represent density flux\n",
    "    mean_flux = np.mean(grad_magnitude)\n",
    "    \n",
    "    # Normalize to area in square micrometers (um^2)\n",
    "    pixel_area = tiles_size ** 2\n",
    "    total_area_um2 = total_image_area * pixel_area\n",
    "    \n",
    "    feature_value = mean_flux / total_area_um2\n",
    "    \n",
    "    return {\"rudy_pin_density_flux_index\": feature_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feat_func_list = [demarcated_macro_proximity_index, macro_surface_irregularity_index, macro_rudy_boundary_interaction_index, pin_density_peak_contrast, rudy_pin_density_flux_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_func_list = [feat_func for feat_func in feat_func_list if feat_func.__name__ in list(feat_pool)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_func_list = feat_func_list + new_feat_func_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.rudy_gradient_variability(images)>,\n",
       " <function __main__.clustered_macro_distance_std(images)>,\n",
       " <function __main__.rudy_pin_clustering_coefficient(images)>,\n",
       " <function __main__.macro_density_gradient(images)>,\n",
       " <function __main__.macro_aspect_ratio_variance(images)>,\n",
       " <function __main__.macro_compactness_index(images)>,\n",
       " <function __main__.macro_variability_coefficient(images)>,\n",
       " <function __main__.macro_symmetry_coefficient(images)>,\n",
       " <function __main__.macro_cluster_density_contrast(images)>,\n",
       " <function __main__.rudy_pin_distribution_kurtosis(images)>,\n",
       " <function __main__.localized_rudy_variability_coefficient(images)>,\n",
       " <function __main__.macro_distribution_clarity_index(images)>,\n",
       " <function __main__.rudy_direction_consistency_index(images)>,\n",
       " <function __main__.rudy_pin_area_masking_index(images)>,\n",
       " <function __main__.rudy_pin_gradient_convergence(images)>,\n",
       " <function __main__.rudy_intensity_symmetry_index(images)>,\n",
       " <function __main__.rudy_deviation_effect_index(images)>,\n",
       " <function __main__.macro_rudy_boundary_interaction_index(images)>,\n",
       " <function __main__.rudy_pin_boundary_pressure_gradient(images)>,\n",
       " <function __main__.edge_macro_overlap_index(images)>,\n",
       " <function __main__.rudy_pin_distribution_smoothness(images)>,\n",
       " <function __main__.rudy_pin_density_gradient_shift(images)>,\n",
       " <function __main__.rudy_pin_distribution_entropy(images)>,\n",
       " <function __main__.rudy_pin_cluster_periphery_density(images)>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_func_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc.update(new_feat_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pool = {\n",
    "    'rudy_gradient_variability': 'the variation in gradient changes across the rudy map indicating potential areas of abrupt routing demand shifts',\n",
    "    'clustered_macro_distance_std': 'the standard deviation of distances between clustered groups of macros',\n",
    "    'rudy_pin_clustering_coefficient': 'a measure of how many rudy pins cluster together relative to the total number of rudy pins',\n",
    "    'macro_density_gradient': 'the change in macro density across the layout, impacting local congestion',\n",
    "    'macro_aspect_ratio_variance': 'the variance in aspect ratios of macros, indicating potential alignment and spacing issues that may impact congestion',\n",
    "    'macro_compactness_index': 'a measure of how closely packed the macros are, potentially affecting routing paths and congestion',\n",
    "    'rudy_pin_compaction_ratio': 'the ratio of compacted rudy pin clusters to the total number of rudy pins, indicating areas with high potential routing conflicts',\n",
    "    'macro_variability_coefficient': 'a measure of the consistency in macro sizes and shapes relative to each other, potentially affecting congestion balance',\n",
    "    'macro_symmetry_coefficient': 'a measure of the symmetry in macro placements relative to the overall layout, potentially influencing uniformity in congestion distribution',\n",
    "    'macro_cluster_density_contrast': 'the contrast in density between clustered groups of macros and their surrounding layout areas, indicating potential localized congestion pressure',\n",
    "    'rudy_pin_distribution_kurtosis': 'a measure of the peakedness or flatness in the distribution of rudy pins across the layout, indicating potential areas of concentrated or dispersed routing demand',\n",
    "    'localized_rudy_variability_coefficient': 'a measure of the variation in RUDY intensity within localized regions, indicating potential micro-level congestion fluctuations',\n",
    "    'macro_distribution_clarity_index': 'a measure of how distinct macro distributions are across the layout, indicating clarity in separation and potential influence on congestion distribution',\n",
    "    'rudy_direction_consistency_index': 'a measure of the uniformity in the directional flow of RUDY intensity, indicating how consistent the routing demand is across the layout',\n",
    "    'rudy_pin_area_masking_index': 'the ratio of the area masked by rudy pin regions relative to the total layout, indicating potential routing blockages',\n",
    "    'rudy_pin_gradient_convergence': 'a measure of how gradients in the rudy pin map converge into specific regions, indicating high-density pin clusters',\n",
    "    'rudy_intensity_symmetry_index': 'a measure of the symmetry in the RUDY intensity map across the layout, indicating uniformity in routing demand distribution',\n",
    "    'rudy_deviation_effect_index': 'a measure of the deviation of RUDY intensities from the mean, indicating areas of abnormal routing demand',\n",
    "    'demarcated_macro_proximity_index': 'a measure of how close macros are to predefined boundary regions, potentially affecting routing and congestion near layout edges',\n",
    "    'macro_surface_irregularity_index': 'a measure of the irregularity in macro surface shapes, which can impact routing paths and layout clarity',\n",
    "    'macro_rudy_boundary_interaction_index': 'a measure of the interaction between macros and high RUDY regions, indicating potential congestion hotspots',\n",
    "    'pin_density_peak_contrast': 'the contrast between peak pin density regions and their surroundings, indicating areas of abrupt routing demand changes',\n",
    "    'rudy_pin_density_flux_index': 'a measure of the rate of change in rudy pin density across the layout, indicating dynamic routing demand shifts',\n",
    "    'high_density_rudy_ratio': 'the ratio of areas with high RUDY intensity to the total layout area, indicating overall routing demand hotspots',\n",
    "    'high_density_rudy_pin_ratio': 'the ratio of areas with high RUDY pin intensity to the total layout area, indicating localized pin density hotspots'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_func_list = [\n",
    " rudy_gradient_variability,\n",
    " clustered_macro_distance_std,\n",
    " rudy_pin_clustering_coefficient,\n",
    " macro_density_gradient,\n",
    " macro_aspect_ratio_variance,\n",
    " macro_compactness_index,\n",
    " rudy_pin_compaction_ratio,\n",
    " macro_variability_coefficient,\n",
    " macro_symmetry_coefficient,\n",
    " macro_cluster_density_contrast,\n",
    " rudy_pin_distribution_kurtosis,\n",
    " localized_rudy_variability_coefficient,\n",
    " macro_distribution_clarity_index,\n",
    " rudy_direction_consistency_index,\n",
    " rudy_pin_area_masking_index,\n",
    " rudy_pin_gradient_convergence,\n",
    " rudy_intensity_symmetry_index,\n",
    " rudy_deviation_effect_index,\n",
    " demarcated_macro_proximity_index,\n",
    " macro_surface_irregularity_index,\n",
    " macro_rudy_boundary_interaction_index,\n",
    " pin_density_peak_contrast,\n",
    " rudy_pin_density_flux_index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function __main__.rudy_gradient_variability(images)>,\n",
       " <function __main__.clustered_macro_distance_std(images)>,\n",
       " <function __main__.rudy_pin_clustering_coefficient(images)>,\n",
       " <function __main__.macro_density_gradient(images)>,\n",
       " <function __main__.macro_aspect_ratio_variance(images)>,\n",
       " <function __main__.macro_compactness_index(images)>,\n",
       " <function __main__.rudy_pin_compaction_ratio(images)>,\n",
       " <function __main__.macro_variability_coefficient(images)>,\n",
       " <function __main__.macro_symmetry_coefficient(images)>,\n",
       " <function __main__.macro_cluster_density_contrast(images)>,\n",
       " <function __main__.rudy_pin_distribution_kurtosis(images)>,\n",
       " <function __main__.localized_rudy_variability_coefficient(images)>,\n",
       " <function __main__.macro_distribution_clarity_index(images)>,\n",
       " <function __main__.rudy_direction_consistency_index(images)>,\n",
       " <function __main__.rudy_pin_area_masking_index(images)>,\n",
       " <function __main__.rudy_pin_gradient_convergence(images)>,\n",
       " <function __main__.rudy_intensity_symmetry_index(images)>,\n",
       " <function __main__.rudy_deviation_effect_index(images)>,\n",
       " <function __main__.demarcated_macro_proximity_index(images)>,\n",
       " <function __main__.macro_surface_irregularity_index(images)>,\n",
       " <function __main__.macro_rudy_boundary_interaction_index(images)>,\n",
       " <function __main__.pin_density_peak_contrast(images)>,\n",
       " <function __main__.rudy_pin_density_flux_index(images)>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_func_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
