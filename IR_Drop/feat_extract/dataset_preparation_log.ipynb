{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initailization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat_extract_log import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_design = [\"RISCY-a\", \"RISCY-b\", \"RISCY-FPU-a\", \"RISCY-FPU-b\"]\n",
    "test_design_a = [\"zero-riscy-a\"]\n",
    "test_design_b = [\"zero-riscy-b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_size = 4\n",
    "top_k = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests, base64\n",
    "import json\n",
    "import argparse\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import heapq\n",
    "import re\n",
    "\n",
    "\n",
    "tile_size = 16\n",
    "top_k = 20\n",
    "image_size = 256\n",
    "\n",
    "\n",
    "def get_label(label_path):\n",
    "    with open(label_path, 'r') as f:\n",
    "        logs = f.read()\n",
    "    matches = re.findall(r\"Total overcon =\\s*([\\d.]+)\", logs)\n",
    "    if matches:\n",
    "        ans = float(matches[-1])\n",
    "    else:\n",
    "        ans = 0\n",
    "        \n",
    "    return ans\n",
    "\n",
    "file_path = '/data2/NVIDIA/CircuitNet-N28/Dataset/congestion/feature/zero-riscy-a/7228-zero-riscy-a-1-c2-u0.9-m2-p4-f0.npy'\n",
    "label_path = '/data2/NVIDIA/CircuitNet-N28/Dataset/logs/7228-zero-riscy-a-1-c2-u0.9-m2-p4-f0'\n",
    "numpy_image = np.load(file_path)\n",
    "batch_image = numpy_image.transpose(2,0,1)\n",
    "image_features = []\n",
    "image_inferences = []\n",
    "\n",
    "for i, image in enumerate(batch_image):\n",
    "    image_features.append(image)\n",
    "    image_inferences.append(Image.fromarray(np.uint8(image * 255)))\n",
    "    \n",
    "get_label(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(logging_file_path):\n",
    "    with open(logging_file_path, \"r\") as f:\n",
    "        logging_file_string = f.read()\n",
    "            \n",
    "    final_features = {}\n",
    "    \n",
    "    for feat_func in feat_func_list:\n",
    "        feat = feat_func(logging_file_string)\n",
    "        final_features.update(feat)\n",
    "        \n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitness Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_wirelength': 2237883.0,\n",
       " 'number_vias': 444319.0,\n",
       " 'number_of_multi_cut_vias': 283.0,\n",
       " 'number_of_single_cut_vias': 444036.0,\n",
       " 'max_overcon': 37.0,\n",
       " 'total_overcon': 24.2,\n",
       " 'worst_layer_gcell_overcon_rate': 41.14,\n",
       " 'hard_to_access_pins_ratio': 0.37574349442379185,\n",
       " 'instance_blockages_count': 76180,\n",
       " 'early_gr_overflow_percentage': 3.39,\n",
       " 'horizontal_overflow_percentage': 0.08,\n",
       " 'congestion_prediction_accuracy': -126.60149253731343,\n",
       " 'initial_placement_efficiency': 0.015246226558926665,\n",
       " 'area_based_congestion_density': 3.7037500000000003,\n",
       " 'multi_layer_pin_access_variability': 38.06713524731668,\n",
       " 'average_layer_congestion': 2.37875,\n",
       " 'pin_density_variance_map': np.float64(146.6786921875),\n",
       " 'non_default_routing_rule_usage': 403,\n",
       " 'crosstalk_sensitive_zones': 20,\n",
       " 'inter_macro_channel_congestion': 58.62}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_features(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "def dataset_setting(designs):\n",
    "    df_list = []\n",
    "    for design in designs:\n",
    "        feature_path = f\"/data2/NVIDIA/CircuitNet-N28/Dataset/congestion/feature/{design}/\" \n",
    "        label_path = f\"/data2/NVIDIA/CircuitNet-N28/Dataset/logs/\"\n",
    "\n",
    "        labels = []\n",
    "        ids = []\n",
    "\n",
    "        for filename in tqdm(os.listdir(feature_path)):\n",
    "            file_path = os.path.join(label_path, filename)\n",
    "            log_file_path = file_path.replace(\".npy\", \"\")\n",
    "            try:\n",
    "                label = get_label(log_file_path)\n",
    "            except:\n",
    "                label = np.nan\n",
    "                \n",
    "            ids.append(filename)\n",
    "            labels.append(label)\n",
    "            \n",
    "        df = pd.DataFrame({\"id\": ids,})\n",
    "\n",
    "        for filename in tqdm(os.listdir(feature_path)):\n",
    "            file_path = os.path.join(label_path, filename)\n",
    "            log_file_path = file_path.replace(\".npy\", \"\")\n",
    "            \n",
    "            index = (df[\"id\"] == filename)\n",
    "            \n",
    "            try:\n",
    "                all_features = get_all_features(log_file_path)\n",
    "            except:\n",
    "                all_features = {}\n",
    "            for key, value in all_features.items():\n",
    "                df.loc[index, key] = value\n",
    "                \n",
    "        \n",
    "        df['label'] = labels\n",
    "        df_list.append(df)\n",
    "        \n",
    "    return pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2003/2003 [00:00<00:00, 12223.06it/s]\n",
      "100%|██████████| 2003/2003 [00:17<00:00, 114.46it/s]\n",
      "100%|██████████| 1858/1858 [00:00<00:00, 6100.24it/s]\n",
      "100%|██████████| 1858/1858 [00:20<00:00, 89.23it/s] \n",
      "100%|██████████| 1969/1969 [00:00<00:00, 5479.00it/s]\n",
      "100%|██████████| 1969/1969 [00:22<00:00, 88.78it/s] \n",
      "100%|██████████| 1248/1248 [00:00<00:00, 4993.75it/s]\n",
      "100%|██████████| 1248/1248 [00:21<00:00, 59.31it/s]\n",
      "100%|██████████| 2042/2042 [00:00<00:00, 14284.30it/s]\n",
      "100%|██████████| 2042/2042 [00:14<00:00, 139.26it/s]\n",
      "100%|██████████| 1122/1122 [00:00<00:00, 11975.43it/s]\n",
      "100%|██████████| 1122/1122 [00:10<00:00, 105.24it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = dataset_setting(train_design)\n",
    "test_df_a = dataset_setting(test_design_a)\n",
    "test_df_b = dataset_setting(test_design_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['label'].notna()]\n",
    "test_df_a = test_df_a[test_df_a['label'].notna()]\n",
    "test_df_b = test_df_b[test_df_b['label'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_df = train_df[[\"id\"] + list(feat_pool.keys()) + [\"label\"]]\n",
    "test_df_a = test_df_a[[\"id\"] + list(feat_pool.keys()) + [\"label\"]]\n",
    "test_df_b = test_df_b[[\"id\"] + list(feat_pool.keys()) + [\"label\"]]\n",
    "train_df[list(feat_pool.keys())] = scaler.fit_transform(train_df[list(feat_pool.keys())])\n",
    "test_df_a[list(feat_pool.keys())] = scaler.fit_transform(test_df_a[list(feat_pool.keys())]) \n",
    "test_df_b[list(feat_pool.keys())] = scaler.fit_transform(test_df_b[list(feat_pool.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df_a.reset_index(drop=True, inplace=True)\n",
    "test_df_b.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5597, 22), (1337, 22), (1122, 22))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df_a.shape, test_df_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/train_df_log.csv\", index=False)\n",
    "test_df_a.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_log_a.csv\", index=False)\n",
    "test_df_b.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_log_b.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/train_df_log.csv\")\n",
    "test_df_a = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_log_a.csv\")\n",
    "test_df_b = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_log_b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df_a.reset_index(drop=True, inplace=True)\n",
    "test_df_b.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_design(name):\n",
    "    for d in train_design:\n",
    "        if d in name:\n",
    "            return d\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"design\"] = train_df[\"id\"].apply(id_to_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1169 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1169 [00:05<20:11,  1.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_a[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m sample_b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     13\u001b[0m     chosen \u001b[38;5;241m=\u001b[39m sample_a[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m     rejected \u001b[38;5;241m=\u001b[39m \u001b[43msample_b\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     15\u001b[0m     chosen_score \u001b[38;5;241m=\u001b[39m sample_a[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     16\u001b[0m     rejected_score \u001b[38;5;241m=\u001b[39m sample_b[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/series.py:1104\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 1104\u001b[0m key_is_scalar \u001b[38;5;241m=\u001b[39m \u001b[43mis_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1106\u001b[0m     key \u001b[38;5;241m=\u001b[39m unpack_1tuple(key)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preference_df_list = []\n",
    "num_pairs = 50000\n",
    "\n",
    "for design, group in train_df.groupby(\"design\"):\n",
    "    preference_df = pd.DataFrame(columns=[\"design\", \"chosen\", \"rejected\", \"chosen_score\", \"rejected_score\"])\n",
    "    group = group.reset_index(drop=True)\n",
    "    num_samples = len(group)\n",
    "    for i in tqdm(range(0, num_samples)):\n",
    "        for j in range(i+1, num_samples):\n",
    "            sample_a = group.iloc[i]\n",
    "            sample_b = group.iloc[j]\n",
    "            if sample_a[\"label\"] > sample_b[\"label\"]:\n",
    "                chosen = sample_a[\"id\"]\n",
    "                rejected = sample_b[\"id\"]\n",
    "                chosen_score = sample_a[\"label\"]\n",
    "                rejected_score = sample_b[\"label\"]\n",
    "            else:\n",
    "                chosen = sample_b[\"id\"]\n",
    "                rejected = sample_a[\"id\"]\n",
    "                chosen_score = sample_b[\"label\"]\n",
    "                rejected_score = sample_a[\"label\"]\n",
    "                \n",
    "            preference_df = preference_df._append({\"design\": design, \"chosen\": chosen, \"rejected\": rejected, \"chosen_score\": chosen_score, \"rejected_score\": rejected_score}, ignore_index=True)\n",
    "            \n",
    "    preference_df = preference_df.sample(frac=1).reset_index(drop=True)\n",
    "    preference_df = preference_df.sample(n=num_pairs)\n",
    "    preference_df_list.append(preference_df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preference_df = pd.concat(preference_df_list)\n",
    "preference_df.reset_index(drop=True, inplace=True)\n",
    "preference_df.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/preference_df_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/train_df.csv\")\n",
    "test_df_a = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_a.csv\")\n",
    "test_df_b = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_b.csv\")\n",
    "\n",
    "train_df_log = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/train_df_log.csv\")\n",
    "test_df_log_a = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_log_a.csv\")\n",
    "test_df_log_b = pd.read_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_log_b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df_log, train_df, on=['id','label'], how='inner').reset_index(drop=True)\n",
    "test_df_a = pd.merge(test_df_log_a, test_df_a, on=['id','label'], how='inner').reset_index(drop=True)\n",
    "test_df_b = pd.merge(test_df_log_b, test_df_b, on=['id','label'], how='inner').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5597, 47), (1337, 47), (1122, 47))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df_a.shape, test_df_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/train_df_mixed.csv\", index=False)\n",
    "test_df_a.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_mixed_a.csv\", index=False)\n",
    "test_df_b.to_csv(\"/home/felixchaotw/mllm-physical-design/armo/dataset/test_df_mixed_b.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
